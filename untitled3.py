# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_-97MVy7hip_zfwEow3PiaAu4ZM0jllp
"""

# Library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#!pip install streamlit plotly
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
#!pip install statsmodels
import statsmodels.api as sm
import statsmodels

meta_rows2 = [
    {"indicator_id": "EMPLOY_RATE_6_12M", "name": "Ù…Ø¹Ø¯Ù„ ØªÙˆØ¸ÙŠÙ Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ† Ø®Ù„Ø§Ù„ 6â€“12 Ø´Ù‡Ø±Ù‹Ø§", "component_details": "Outcomes-Employment","component": "Outcomes","component": "Outcomes", "weight": 0.18, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "EMPLOYER_SATISFACTION", "name": "Ù…Ø¹Ø¯Ù„ Ø±Ø¶Ø§ Ø£ØµØ­Ø§Ø¨ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ†", "component_details": "Outcomes-Employment","component": "Outcomes", "weight": 0.14, "function": "Concurrent", "goal": 1, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "PROGRAMS_INTL_RANKED", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…ØµÙ†ÙØ© Ø¯ÙˆÙ„ÙŠÙ‹Ø§", "component_details": "Outcomes-Employment","component": "Outcomes",  "weight": 0.03, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "CAREER_SERVICES", "name": "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø±ÙŠØ§Ø¯ÙŠØ© Ø§Ù„Ù…Ø­ØªØ¶Ù†Ø©/Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©", "component_details": "Outcomes-Innovation& entrepreneurship", "component": "Outcomes", "weight": 0.04, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "PATENT_COUNT", "name": "Ø¹Ø¯Ø¯ Ø¨Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø§Ø®ØªØ±Ø§Ø¹", "component_details": "Outcomes-Innovation& entrepreneurship","component": "Outcomes", "weight": 0.02, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "BIZ_PARTNERSHIPS", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø·Ù„Ø¨Ø© ÙÙŠ Ø¨Ø±Ø§Ù…Ø¬ STEMM (Ù…Ø¹ Ø§Ù„ØµØ­Ø©)", "component_details": "Processes- Education&traning",  "component": "Processess","weight": 0.04, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "NATIONAL_PROF_EXAMS", "name": "Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ ÙÙŠ Ù…ÙˆØ§Ø¯ Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©", "component_details": "Processes- Education&traning", "component": "Processess", "weight": 0.06, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "LINKEDIN_ACTIVE", "name": "Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ®Ø±Ø¬ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯", "component_details": "Processes- Education&traning", "component": "Processess", "weight": 0.06, "function": "Lagging", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "WAGE_GROWTH_1Y", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ© Ø§Ù„Ù…ÙˆØ­Ø¯Ø©", "component_details": "Processes- Education&traning",  "component": "Processess","weight":0.06, "function": "Lagging", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "FOUNDERS_RATE", "name": "Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ÙˆØ·Ù†ÙŠØ©/Ø§Ù„Ù…Ù‡Ù†ÙŠØ©", "component_details": "Processes- Education&traning", "component": "Processess", "weight": 0.05, "function": "Lagging", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "PROMOTION_RATE", "name": "Ø¯Ø¹Ù… Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ† Ø¨Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ù…Ù‡Ù†ÙŠ", "component_details": "Processes- Support & empowerment", "component": "Processess", "weight": 0.04, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "JOB_STABILITY_2Y", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ù…Ø¹ Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„", "component_details": "Processes- Support & empowerment",  "component": "Processess","weight": 0.1, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "STEMM_SHARE", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø·Ù„Ø¨Ø© Ø§Ù„Ù…Ø´Ø§Ø±ÙƒÙŠÙ† ÙÙŠ Ù…Ø¨Ø§Ø¯Ø±Ø§Øª Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±", "component_details": "Processes- Support & empowerment", "component": "Processess", "weight": 0.04, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "ACCRED_PROGS", "name": "Ø±Ø¶Ø§ Ø§Ù„Ø·Ù„Ø¨Ø© Ø¹Ù† Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©", "component_details": "Processes- Support & empowerment", "component": "Processess", "weight": 0.03, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "INTERN_RATE", "name": "Ø±Ø¶Ø§ Ø§Ù„Ø·Ù„Ø¨Ø© Ø¹Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø³Ø§Ù†Ø¯Ø©", "component_details": "Processes- Support & empowerment", "component": "Processess", "weight": 0.03, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "ENTRE_SUPPORT", "name": "Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø§Ù„Ù…Ø¹Ø²Ø²Ø© Ù„Ù…ÙÙ‡ÙˆÙ… Ø±ÙŠØ§Ø¯Ø© Ø§Ù„Ø£Ø¹Ù…Ø§Ù„", "component_details": "Processes-Supportive academic programs","component": "Processes", "weight": 0.03, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "FUNDING_SHARE", "name": "Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ø¨Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø±Ù† 21", "component_details": "Processes-Supportive academic programs",  "component": "Processess","weight": 0.03, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "STUDENT_SAT", "name": "Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ø¨Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¯Ø§Ù…Ø©", "component_details": "Processes-Supportive academic programs", "component": "Processess","weight": 0.03, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "GRAD_SAT", "name": "Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ù‡Ø§Ø±ÙŠ (Skills Portfolio)", "component_details": "Conceptual-Human","component": "Conceptual",  "weight": 0.02, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "SKILL_DEV", "name": "Ø§Ù„Ø´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©", "component_details": "Conceptual-Human", "component": "Conceptual", "weight": 0.02, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "RESEARCH_PUBS", "name": "Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ù…Ø¤ØªÙ…Ø±Ø§Øª/Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø§Øª", "component_details": "Conceptual-Human","component": "Conceptual",  "weight": 0.02, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "INTERNATIONAL_STUD", "name": "Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ø£Ù†Ø¯ÙŠØ© Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠØ©", "component_details": "Conceptual-Social &professional", "component": "Conceptual", "weight": 0.01, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "FACULTY_INTL", "name": "Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„ØªØ·ÙˆØ¹ ÙˆØ®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¬ØªÙ…Ø¹", "component_details": "Conceptual-Social &professional","component": "Conceptual",  "weight": 0.01, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "MOBILITY", "name": "Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ù‡Ù†ÙŠØ© Ø§Ù„Ù†Ø´Ø·Ø© (LinkedIn)", "component_details": "Conceptual-Social &professional", "component": "Conceptual", "weight": 0.04, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "NATIONAL_PART", "name": "ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ùˆ Ù„ØºØ§Øª Ø£Ø¬Ù†Ø¨ÙŠØ©", "component_details": "Conceptual-Human", "component": "Conceptual", "weight": 0.02, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "CURRICULUM_UPD", "name": "Ù…ØªÙˆØ³Ø· Ù†Ø³Ø¨Ø© Ø§Ù„Ø²ÙŠØ§Ø¯Ø© ÙÙŠ Ø§Ù„Ø±ÙˆØ§ØªØ¨ Ø¨Ø¹Ø¯ Ø³Ù†Ø©", "component_details": "Impact","component": "Impact",  "weight": 0.12, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "ENTRE_SUPPORT", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ† Ø§Ù„Ù…Ø¤Ø³Ø³ÙŠÙ† Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø±ÙŠØ§Ø¯ÙŠØ©", "component_details": "Impact","component": "Impact",  "weight": 0.06, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "EMPLOYABILITY_SKILL", "name": "Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ±Ù‚ÙŠØ§Øª Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ© Ù„Ù„Ø®Ø±ÙŠØ¬ÙŠÙ†", "component_details": "Impact", "component": "Impact", "weight": 0.06, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "COMM_SKILLS", "name": "Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„ÙˆØ¸ÙŠÙÙŠ", "component_details": "Impact","component": "Impact",  "weight": 0.06, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "DIGITAL_SKILLS", "name": "Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©", "component_details": "Impact","component": "Impact",  "weight": 0.04, "function": "Lagging", "goal": 1, "direction": "HigherIsBetter", "is_likert": True},
]

indicators_meta2 = pd.DataFrame(meta_rows2)
#indicators_meta2
# Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø­ÙŠØ« ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ = 1
indicators_meta2["weight"] = indicators_meta2["weight"] / indicators_meta2["weight"].sum()

# Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
print("New total for Weight =", indicators_meta2["weight"].sum())

# -----------------------------

#SAMUILATE
np.random.seed(42)
years = [2021, 2022, 2023, 2024]
universities = ["UniA", "UniB", "UniC"]

def likert_to_100(x): return ((x - 1) / 4.0) * 100.0

def minmax_0_100(series, higher_is_better=True):
    s = series.astype(float)
    vmin, vmax = s.min(), s.max()
    norm = (s - vmin) / (vmax - vmin) * 100.0 if vmax > vmin else pd.Series(50.0, index=s.index)
    return norm if higher_is_better else 100.0 - norm


def slope(series):
    years_sorted = sorted(series.keys())
    diffs = [series[b]-series[a] for a,b in zip(years_sorted[:-1], years_sorted[1:])]
    return np.mean(diffs) if diffs else np.nan

def classify_trend(s):
    if pd.isna(s): return "N/A"
    if s >= 5: return "Strong Uptrend"
    if 2 <= s < 5: return "Mild Uptrend"
    if -1.9 <= s <= 1.9: return "Flat"
    if -5 < s < -2: return "Mild Downtrend"
    if s <= -5: return "Strong Downtrend"
    return "Flat"

# --------------------------
# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„ÙƒÙ„ Ù†ÙˆØ¹
# --------------------------
np.random.seed(42)
if 'value' not in indicators_meta2.columns:
    indicators_meta2['value'] = np.random.rand(len(indicators_meta2))

indices_df = (
    indicators_meta2
      .assign(wv = indicators_meta2['value'] * indicators_meta2['weight'])
      .groupby('function', as_index=False)['wv'].sum()
      .rename(columns={'wv': 'Index'})
)

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
overall_index = indices_df['Index'].sum()
indices_df = pd.concat([indices_df, pd.DataFrame([{'function': 'Overall', 'Index': overall_index}])], ignore_index=True)

print(indices_df)

# Functions

np.random.seed(42)
years = [2021, 2022, 2023, 2024]
universities = ["UniA", "UniB", "UniC"]

def likert_to_100(x): return ((x - 1) / 4.0) * 100.0

def minmax_0_100(series, higher_is_better=True):
    s = series.astype(float)
    vmin, vmax = s.min(), s.max()
    norm = (s - vmin) / (vmax - vmin) * 100.0 if vmax > vmin else pd.Series(50.0, index=s.index)
    return norm if higher_is_better else 100.0 - norm


def slope(series):
    years_sorted = sorted(series.keys())
    diffs = [series[b]-series[a] for a,b in zip(years_sorted[:-1], years_sorted[1:])]
    return np.mean(diffs) if diffs else np.nan

def slope_from_series(year_to_value: dict) -> float:
    """Ù…ÙŠÙ„ Ø¨Ø³ÙŠØ· Ø¹Ø¨Ø± ÙØ±ÙˆÙ‚Ø§Øª Ø³Ù†ÙˆÙŠØ© Ù…ØªØªØ§Ù„ÙŠØ©."""
    if not year_to_value:
        return np.nan
    years_sorted = sorted(year_to_value.keys())
    diffs = [year_to_value[b] - year_to_value[a] for a,b in zip(years_sorted[:-1], years_sorted[1:])]
    return float(np.mean(diffs)) if diffs else np.nan

def classify_trend5(s):
    if pd.isna(s): return "Steady"
    if s >= 5: return "High Upper Trend"
    if 2 <= s < 5: return "Moderate Upper Trend"
    if -1.9 <= s <= 1.9: return "Steady"
    if -5 < s < -2: return "Moderate Lower Trend"
    if s <= -5: return "High Lower Trend"
    return "Steady"

# Ù„ØªÙØ³ÙŠØ± "Steady" Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ (Ù…Ø±ØªÙØ¹/Ù…ØªÙˆØ³Ø·/Ù…Ù†Ø®ÙØ¶) Ø¥Ù† Ø§Ø­ØªØ¬Øª
def level_bucket(x):
    if pd.isna(x): return "Unknown"
    if x >= 75: return "High"
    if x >= 50: return "Mid"
    return "Low"

def aggregate(df, by_cols):
    grouped = []
    for keys, sub in df.groupby(by_cols):
        v = sub["norm_0_100"].to_numpy(dtype=float)
        w = sub["weight"].to_numpy(dtype=float)

        den = w.sum()
        if den == 0:
            raise ValueError(f"Ø§Ù„Ø£ÙˆØ²Ø§Ù† = 0 ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© {keys} â†’ Ø´ÙŠ ØºÙ„Ø· Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

        val = (v @ w) / den  # Ø­Ø§ØµÙ„ Ø¶Ø±Ø¨ Ø¯Ø§Ø®Ù„ÙŠ Ã· Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†

        rec = dict(zip(by_cols, keys if isinstance(keys, tuple) else (keys,)))
        rec["score"] = val
        grouped.append(rec)
    return pd.DataFrame(grouped)

# -----------------------------
# 3) Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§Ù…
# -----------------------------
raw = []
rng = np.random.default_rng(2025)
for uni in universities:
    for y in years:
        for _, row in indicators_meta2.iterrows():
            if row["is_likert"]:
                val = rng.uniform(2.5, 4.7)  # Ù„ÙŠÙƒØ±Øª 1â€“5
            else:
                val = rng.uniform(40, 95)    # Ù†Ø³Ø¨ Ù…Ø¦ÙˆÙŠØ©
            raw.append((uni, y, row["indicator_id"], val))
raw_df = pd.DataFrame(raw, columns=["university","year","indicator_id","raw_value"])

# -----------------------------
# 4) Ø§Ù„ØªØ·Ø¨ÙŠØ¹
# -----------------------------
norm_rows = []
for ind, sub in raw_df.groupby("indicator_id"):
    meta = indicators_meta2[indicators_meta2["indicator_id"]==ind].iloc[0]
    higher = (meta["direction"] != "LowerIsBetter")
    sub = sub.copy()
    if meta["is_likert"]:
        sub["norm_0_100"] = likert_to_100(sub["raw_value"])
    else:
        sub["norm_0_100"] = minmax_0_100(sub["raw_value"], higher)
    norm_rows.append(sub)
norm_df = pd.concat(norm_rows)


# -----------------------------
# 5)
# -----------------------------
merged = norm_df.merge(indicators_meta2, on="indicator_id")
merged["weight"] = merged["weight"].fillna(0.0)

# -----------------------------
# 6)
# -----------------------------

# 1) Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‡Ø¯Ù (weighted by indicator weights)
func_level = (
    merged
    .assign(wv = merged["norm_0_100"] * merged["weight"])
    .groupby(["university","year","goal","function"], as_index=False)
    .agg(score_sum=("wv","sum"), wsum=("weight","sum"))
    .assign(score = lambda d: np.where(d["wsum"]>0, d["score_sum"]/d["wsum"], np.nan))
    .drop(columns=["score_sum"])
)

# 2) Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù‡Ø¯Ù (weighted by total weights carried from functions)
goal_level = (
    func_level
    .assign(wv = func_level["score"] * func_level["wsum"])
    .groupby(["university","year","goal"], as_index=False)
    .agg(score_sum=("wv","sum"), wsum=("wsum","sum"))
    .assign(score = lambda d: np.where(d["wsum"]>0, d["score_sum"]/d["wsum"], np.nan))
    .drop(columns=["score_sum"])
)

# 3) Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ (weighted by total weights carried from goals)
overall = (
    goal_level
    .assign(wv = goal_level["score"] * goal_level["wsum"])
    .groupby(["university","year"], as_index=False)
    .agg(score_sum=("wv","sum"), wsum=("wsum","sum"))
    .assign(overall_index_0_100 = lambda d: np.where(d["wsum"]>0, d["score_sum"]/d["wsum"], np.nan))
    .drop(columns=["score_sum","wsum"])
)

# ===============================
# 1) Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ²ÙˆÙ† Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„
#    (ÙŠØ­Ù…Ù„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø­ØªÙ‰ Ø§Ù„Ù‚Ù…Ø©)
# ===============================
# merged ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ columns: ['university','year','goal','function','indicator_id','norm_0_100','weight', ...]
# ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø£ÙˆØ²Ø§Ù† NaN
if merged['weight'].isna().any():
    raise ValueError("ÙŠÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø¨Ù„Ø§ ÙˆØ²Ù†. Ø¹Ø§Ù„Ø¬Ù‡Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© (Ù…Ø«Ù„Ø§Ù‹ fillna Ø£Ùˆ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø£ÙˆØ²Ø§Ù†).")

# Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‡Ø¯Ù: ÙˆØ²Ù† Ø¨Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª
func_level = (
    merged
    .assign(wv = merged["norm_0_100"] * merged["weight"])
    .groupby(["university","year","goal","function"], as_index=False)
    .agg(score_sum=("wv","sum"), wsum=("weight","sum"))
    .assign(score = lambda d: np.where(d["wsum"]>0, d["score_sum"]/d["wsum"], np.nan))
    .drop(columns=["score_sum"])
)

# Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù‡Ø¯Ù: ÙˆØ²Ù† Ø¨Ù…Ø¬Ø§Ù…ÙŠØ¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦Ù (Ø§Ù„Ù…Ø­Ù…ÙˆÙ„Ø© Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª)
goal_level = (
    func_level
    .assign(wv = func_level["score"] * func_level["wsum"])
    .groupby(["university","year","goal"], as_index=False)
    .agg(score_sum=("wv","sum"), wsum=("wsum","sum"))
    .assign(score = lambda d: np.where(d["wsum"]>0, d["score_sum"]/d["wsum"], np.nan))
    .drop(columns=["score_sum"])
)

# Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙƒÙ„ÙŠ: ÙˆØ²Ù† Ø¨Ù…Ø¬Ø§Ù…ÙŠØ¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
overall = (
    goal_level
    .assign(wv = goal_level["score"] * goal_level["wsum"])
    .groupby(["university","year"], as_index=False)
    .agg(score_sum=("wv","sum"), wsum=("wsum","sum"))
    .assign(overall_index_0_100 = lambda d: np.where(d["wsum"]>0, d["score_sum"]/d["wsum"], np.nan))
    .drop(columns=["score_sum","wsum"])
)

# ===============================
# 2) Ù…ØµÙÙˆÙØ© (Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ã— Ø§Ù„ÙˆØ¸ÙŠÙØ©) Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø±Ø¤Ù‰
# ===============================
MATRIX = {
    ("Leading",   "High Upper Trend"):   {"ar_i":"ÙØ±ØµØ© Ù‚ÙˆÙŠØ© Ù‚Ø§Ø¯Ù…Ø©","ar_a":"Ø¶Ø§Ø¹Ù Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± ÙˆØ¹Ø¬Ù‘Ù„ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø±Ø§Øª Ø§Ù„ÙˆÙ‚Ø§Ø¦ÙŠØ©/Ø§Ù„ØªÙ…ÙƒÙŠÙ†ÙŠØ©","en_i":"Strong upcoming opportunity","en_a":"Double down on investments and accelerate enabling programs","prio":"High"},
    ("Leading",   "Moderate Upper Trend"):{"ar_i":"ÙØ±ØµØ© Ù…Ø­ØªÙ…Ù„Ø©","ar_a":"Ø±Ø§Ù‚Ø¨ Ø¹Ù† ÙƒØ«Ø¨ ÙˆÙ‚Ø¯Ù‘Ù… Ø¯Ø¹Ù…Ø§Ù‹ Ù…Ø³ØªÙ‡Ø¯ÙØ§Ù‹","en_i":"Potential opportunity","en_a":"Monitor closely and provide targeted support","prio":"Medium"},
    ("Leading",   "Steady"):              {"ar_i":"Ù„Ø§ Ø¥Ø´Ø§Ø±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©","ar_a":"Ø§Ø³ØªÙ…Ø± Ø¨Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø¯ÙˆØ±ÙŠ","en_i":"No new signals","en_a":"Maintain monitoring and periodic checks","prio":"Low"},
    ("Leading",   "Moderate Lower Trend"):{ "ar_i":"ØªÙ‡Ø¯ÙŠØ¯ Ù…Ø­ØªÙ…Ù„","ar_a":"Ø¬Ù‡Ù‘Ø² Ø®Ø·Ø· Ø§Ø­ØªÙˆØ§Ø¡ Ù…Ø¨ÙƒØ± ÙˆØ­Ø¯Ù‘Ø¯ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø¬Ø°Ø±ÙŠØ©","en_i":"Potential threat","en_a":"Prepare early containment plans and identify root causes","prio":"High"},
    ("Leading",   "High Lower Trend"):   {"ar_i":"Ø¥Ù†Ø°Ø§Ø± Ù…Ø¨ÙƒØ± Ù‚ÙˆÙŠ","ar_a":"ØªØ¯Ø®Ù‘Ù„ ÙÙˆØ±ÙŠ Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù…Ø³Ø§Ø± ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯","en_i":"Strong early-warning signal","en_a":"Immediate intervention to correct course and reallocate resources","prio":"Critical"},
    ("Concurrent","High Upper Trend"):   {"ar_i":"Ø£Ø¯Ø§Ø¡ Ø­Ø§Ù„ÙŠ Ù…Ù…ØªØ§Ø²","ar_a":"Ø¹Ù…Ù‘Ù… Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª ÙˆØ§Ø³ØªØ«Ù…Ø± Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰","en_i":"Excellent current performance","en_a":"Scale best practices and invest to sustain level","prio":"High"},
    ("Concurrent","Moderate Upper Trend"):{"ar_i":"ØªØ­Ø³Ù† Ù…Ø³ØªÙ…Ø±","ar_a":"ÙˆØ§ØµÙ„ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ­Ø³Ù‘Ù† ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹","en_i":"Continuous improvement","en_a":"Continue support and drive incremental improvement","prio":"Medium"},
    ("Concurrent","Steady"):             {"ar_i":"Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„ÙˆØ¶Ø¹","ar_a":"Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¶Ø¹ ÙˆØ±Ø§Ù‚Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø®Ø§Ø·Ø±","en_i":"Stable situation","en_a":"Maintain status and monitor risk indicators","prio":"Low"},
    ("Concurrent","Moderate Lower Trend"):{"ar_i":"Ø¶Ø¹Ù Ø¢Ù†ÙŠ","ar_a":"Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆØ­Ø¯Ù‘Ø¯ Ø§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„ÙŠØ©","en_i":"Current weakening","en_a":"Address immediate causes and close operational gaps","prio":"High"},
    ("Concurrent","High Lower Trend"):   {"ar_i":"ØªØ¯Ù‡ÙˆØ± Ø­Ø§Ø¯ Ø§Ù„Ø¢Ù†","ar_a":"Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¹Ø§Ø¬Ù„Ø© ÙˆØ®Ø·Ø© ØªØµØ­ÙŠØ­ Ù‚ØµÙŠØ±Ø© Ø§Ù„Ø£Ø¬Ù„","en_i":"Acute current deterioration","en_a":"Urgent response and short-term recovery plan","prio":"Critical"},
    ("Lagging",  "High Upper Trend"):    {"ar_i":"Ù†ØªØ§Ø¦Ø¬ Ù‚ÙˆÙŠØ© Ù…ØªØ­Ù‚Ù‚Ø©","ar_a":"ÙˆØ«Ù‘Ù‚ Ø§Ù„Ù†Ø¬Ø§Ø­Ø§Øª ÙˆØ¹Ù…Ù‘Ù… Ø§Ù„Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„ÙØ¹Ù‘Ø§Ù„Ø©","en_i":"Strong realized results","en_a":"Document wins and scale effective policies","prio":"Medium"},
    ("Lagging",  "Moderate Upper Trend"):{ "ar_i":"Ø£Ø«Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠ ÙˆØ§Ø¶Ø­","ar_a":"Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆØ­Ø³Ù‘Ù† Ø§Ù„ÙƒÙØ§Ø¡Ø©","en_i":"Clear positive impact","en_a":"Sustain strategy and improve efficiency","prio":"Medium"},
    ("Lagging",  "Steady"):              {"ar_i":"Ø£Ø«Ø± Ù…Ø³ØªÙ‚Ø±","ar_a":"Ù„Ø§ ØªØºÙŠÙŠØ±Ø§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©Ø› Ø±Ø§Ù‚Ø¨ Ù„ØªØ­ÙˆÙ‘Ù„Ø§Øª Ù…Ø­ØªÙ…Ù„Ø©","en_i":"Stable impact","en_a":"No strategic changes; keep watch for shifts","prio":"Low"},
    ("Lagging",  "Moderate Lower Trend"):{ "ar_i":"Ø£Ø«Ø± Ø¶Ø¹ÙŠÙ Ø¨Ø¯Ø£ ÙŠØ¸Ù‡Ø±","ar_a":"Ù‚ÙŠÙ‘Ù… Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙ†ÙÙŠØ° ÙˆØ§Ø±ØªØ¨Ø§Ø·Ù‡ Ø¨Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª","en_i":"Emerging weak impact","en_a":"Evaluate execution quality and input-output linkage","prio":"High"},
    ("Lagging",  "High Lower Trend"):    {"ar_i":"Ø£Ø«Ø± Ø³Ù„Ø¨ÙŠ Ù…Ø¤ÙƒØ¯","ar_a":"Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ÙˆÙ…ÙƒÙˆÙ†Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°","en_i":"Confirmed negative impact","en_a":"Comprehensive review of strategy and execution","prio":"Critical"},
}
PRIO_SCORE = {"Critical":3, "High":2, "Medium":1, "Low":0}

# ===============================
# 3) Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø®Ù…Ø§Ø³ÙŠ ÙˆØ¥Ø³Ù‚Ø§Ø· Ø§Ù„Ù…ØµÙÙˆÙØ©
#    (Ø£) Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ã— Ø§Ù„ÙˆØ¸ÙŠÙØ©
# ===============================
func_insights = []
for (uni, fn), sub in func_level.groupby(["university","function"]):
    # Ù…ÙŠÙ„ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª
    y2v = {int(r["year"]): float(r["score"]) for _, r in sub.sort_values("year").iterrows()}
    s = slope_from_series(y2v)
    t5 = classify_trend5(s)
    # Ø¢Ø®Ø± Ø³Ù†Ø©/Ù‚ÙŠÙ…Ø© Ù„Ø¹Ø±Ø¶Ù‡Ø§
    last_row = sub.loc[sub["year"].idxmax()]
    last_year = int(last_row["year"])
    last_score = float(last_row["score"])

    item = MATRIX.get((fn, t5), None)
    if item is None:
        # Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ ØªÙ‡Ø¬Ø¦Ø© Ù…Ø®ØªÙ„ÙØ© Ù„Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ØªØ£ÙƒØ¯ Ù…Ù†Ù‡Ø§
        item = {"ar_i":"â€”","ar_a":"â€”","en_i":"â€”","en_a":"â€”","prio":"Low"}

    func_insights.append({
        "university": uni,
        "function": fn,
        "trend5": t5,
        "slope": s,
        "latest_year": last_year,
        "latest_score": last_score,
        "interpretation_ar": item["ar_i"],
        "action_ar": item["ar_a"],
        "interpretation_en": item["en_i"],
        "action_en": item["en_a"],
        "priority": item["prio"],
        "priority_score": PRIO_SCORE[item["prio"]],
    })
func_insights_df = pd.DataFrame(func_insights).sort_values(
    ["university","priority_score","function"], ascending=[True, False, True]
).reset_index(drop=True)

# ===============================
#    (Ø¨) Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ã— Ø§Ù„Ù‡Ø¯Ù Ã— Ø§Ù„ÙˆØ¸ÙŠÙØ©
# ===============================
func_goal_insights = []
for (uni, goal, fn), sub in func_level.groupby(["university","goal","function"]):
    y2v = {int(r["year"]): float(r["score"]) for _, r in sub.sort_values("year").iterrows()}
    s = slope_from_series(y2v)
    t5 = classify_trend5(s)
    last_row = sub.loc[sub["year"].idxmax()]
    last_year = int(last_row["year"]); last_score = float(last_row["score"])

    item = MATRIX.get((fn, t5), {"ar_i":"â€”","ar_a":"â€”","en_i":"â€”","en_a":"â€”","prio":"Low"})
    func_goal_insights.append({
        "university": uni, "goal": int(goal), "function": fn,
        "trend5": t5, "slope": s,
        "latest_year": last_year, "latest_score": last_score,
        "interpretation_ar": item["ar_i"], "action_ar": item["ar_a"],
        "interpretation_en": item["en_i"], "action_en": item["en_a"],
        "priority": item["prio"], "priority_score": PRIO_SCORE[item["prio"]],
    })
func_goal_insights_df = pd.DataFrame(func_goal_insights).sort_values(
    ["university","goal","priority_score"], ascending=[True, True, False]
).reset_index(drop=True)

# ===============================
#    (Ø¬) Ø§ØªØ¬Ø§Ù‡ Ù„Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙƒÙ„ÙŠ ÙˆØ§Ù„Ù‡Ø¯Ù ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† ÙˆØ¸ÙŠÙØ©)
# ===============================
# Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ÙƒÙ„ÙŠ:
overall_trends = []
for uni, sub in overall.groupby("university"):
    y2v = {int(r["year"]): float(r["overall_index_0_100"]) for _, r in sub.sort_values("year").iterrows()}
    s = slope_from_series(y2v)
    t5 = classify_trend5(s)
    last_row = sub.loc[sub["year"].idxmax()]
    overall_trends.append({
        "university": uni,
        "trend5": t5,
        "slope": s,
        "latest_year": int(last_row["year"]),
        "overall_now": float(last_row["overall_index_0_100"]),
        "level_bucket": level_bucket(float(last_row["overall_index_0_100"]))
    })
overall_insights_df = pd.DataFrame(overall_trends)

# Ø§ØªØ¬Ø§Ù‡ Ù„ÙƒÙ„ Ù‡Ø¯Ù:
goal_trends = []
for (uni, goal), sub in goal_level.groupby(["university","goal"]):
    y2v = {int(r["year"]): float(r["score"]) for _, r in sub.sort_values("year").iterrows()}
    s = slope_from_series(y2v)
    t5 = classify_trend5(s)
    last_row = sub.loc[sub["year"].idxmax()]
    goal_trends.append({
        "university": uni, "goal": int(goal),
        "trend5": t5, "slope": s,
        "latest_year": int(last_row["year"]),
        "score_now": float(last_row["score"]),
        "level_bucket": level_bucket(float(last_row["score"]))
    })
goal_insights_df = pd.DataFrame(goal_trends)

# ===============================
# 4) Ù…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ Ø³Ø±ÙŠØ¹ (Ù†Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„ÙƒÙ„ Ø¬Ø§Ù…Ø¹Ø©)
# ===============================
def make_executive_brief(uni):
    parts = []
    row = overall_insights_df[overall_insights_df["university"]==uni]
    if not row.empty:
        t = row.iloc[0]
        parts.append(f"Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ: {t['overall_now']:.1f}/100 ({t['trend5']}, Ù…Ø³ØªÙˆÙ‰ {t['level_bucket']}).")
    # Ø£Ù‡Ù… 3 ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©
    top3 = func_insights_df[func_insights_df["university"]==uni].nlargest(3, "priority_score")
    if not top3.empty:
        for _, r in top3.iterrows():
            parts.append(f"- {r['function']} â€¢ {r['trend5']} â€¢ {r['interpretation_ar']} â†’ {r['action_ar']}")
    return "\n".join(parts) if parts else "Ù„Ø§ ØªØªÙˆÙØ± Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©."

# Ù…Ø«Ø§Ù„ Ø·Ø¨Ø§Ø¹Ø© Ù…ÙˆØ¬Ø² Ù„ÙƒÙ„ Ø¬Ø§Ù…Ø¹Ø©:
for u in sorted(overall["university"].unique()):
    print(f"\nğŸ§­ {u}\n{make_executive_brief(u)}")


# ===============================
# 5) (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) ØªÙƒØ§Ù…Ù„ Ø³Ø±ÙŠØ¹ Ù…Ø¹ Streamlit
# ===============================
# ======== IMPORTS Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© ========
import numpy as np, pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# ======== Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø© ========
st.set_page_config(page_title="Overall Graduate Readiness Index", layout="wide")
st.title("ğŸ“ˆ Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯: Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ†")
st.caption("Gauge Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€¢ Pie Ù„Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù â€¢ Trend Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª â€¢ ğŸ‘¨â€ğŸ“ Ø¹Ø¯Ù‘Ø§Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ + ØªÙ†Ø¨Ø¤ ÙˆØ·Ù†ÙŠ")

# ======== ØªØ¬Ù‡ÙŠØ² Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù„Ø§Ø¨ (Ø¥Ù† Ù„Ù… ØªØªÙˆÙØ±) ========
if 'students_df' not in globals() or students_df is None or students_df.empty:
    # Ù†Ø¨Ù†ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ø·Ù„Ø§Ø¨ ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø¬Ø§Ù…Ø¹Ø§Øª ÙˆØ³Ù†ÙˆØ§Øª overall
    rng = np.random.default_rng(42)
    universities_ = sorted(overall["university"].unique().tolist())
    years_ = sorted(overall["year"].unique().tolist())
    srows = []
    for y in years_:
        national_total = int(rng.integers(24000, 36000))
        shares = rng.random(len(universities_)); shares /= shares.sum()
        for u, sh in zip(universities_, shares):
            srows.append((u, y, int(national_total * sh)))
    students_df = pd.DataFrame(srows, columns=["university","year","students"])

# ======== 1) Ø§Ù„Ù…Ø±Ø´Ø­Ø§Øª ========
left, right = st.columns([1,1])
with left:
    universities = sorted(overall["university"].unique().tolist())
    uni = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©", universities, index=0)
with right:
    years = sorted(overall["year"].unique().tolist())
    year_start, year_end = st.select_slider("Ù†Ø·Ø§Ù‚ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ù„Ù„ØªØ±Ù†Ø¯", options=years, value=(years[0], years[-1]))
current_year = year_end

# Ø¥Ø·Ø§Ø±Ø§Øª Ù…Ø±Ø´Ù‘Ø­Ø©
overall_f = overall[(overall["university"]==uni)].sort_values("year")
func_f = func_level[(func_level["university"]==uni)]

# ======== Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù (Ø­Ø³Ø¨ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†) ========
if 'indicators_meta2' in globals() and not indicators_meta2.empty:
    type_weights = (indicators_meta2.groupby("function", as_index=False)["weight"]
                    .sum().rename(columns={"weight":"w_sum"}))
else:
    # fallback Ù„Ùˆ Ù…Ø§ Ø¹Ù†Ø¯Ùƒ indicators_meta2
    type_weights = func_f.groupby("function", as_index=False)["score"].count()
    type_weights = type_weights.rename(columns={"score":"w_sum"})

type_score_now = (func_f[func_f["year"]==current_year]
                  .groupby("function", as_index=False)["score"].mean())
type_breakdown = type_weights.merge(type_score_now, on="function", how="left").fillna({"score":0})

# ======== Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© ========
overall_now = overall_f.loc[overall_f["year"]==current_year, "overall_index_0_100"]
overall_now = float(overall_now.iloc[0]) if len(overall_now) else np.nan

# ======== 2) ğŸ‘¨â€ğŸ“ Ù‚Ø³Ù… Ø§Ù„Ø·Ù„Ø§Ø¨ ========
st.markdown("### ğŸ‘¨â€ğŸ“ Ø§Ù„Ø·Ù„Ø§Ø¨")
students_in_range = students_df[(students_df["year"]>=year_start) & (students_df["year"]<=year_end)]
national_total_range = int(students_in_range["students"].sum()) if not students_in_range.empty else 0

uni_current_students = int(
    students_df[(students_df["university"]==uni) & (students_df["year"]==current_year)]["students"].sum()
) if not students_df.empty else 0

m1, m2, m3 = st.columns(3)
with m1:
    st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ (ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø®ØªØ§Ø±)", f"{national_total_range:,}")
with m2:
    st.metric(f"Ø·Ù„Ø§Ø¨ {uni} ÙÙŠ {current_year}", f"{uni_current_students:,}")
with m3:
    national_current_total = int(students_df[students_df["year"]==current_year]["students"].sum()) if not students_df.empty else 0
    share = (uni_current_students / national_current_total * 100.0) if national_current_total>0 else 0.0
    st.metric("Ø­ØµØ© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ù†Ø©", f"{share:.1f}%")

st.write(f"**ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© â€“ {current_year}**")
students_current_year = students_df[students_df["year"]==current_year].copy()
if not students_current_year.empty:
    bar_students = px.bar(students_current_year, x="university", y="students", text="students",
                          title=f"Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ({current_year})")
    bar_students.update_layout(yaxis_title="Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨", xaxis_title="Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©")
    bar_students.update_traces(texttemplate="%{text:,}", textposition="outside")
    st.plotly_chart(bar_students, use_container_width=True)
else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø·Ù„Ø§Ø¨ Ù„Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.")

# ======== 3) Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ ========
c1, c2 = st.columns([1,1])

# 3.1 Gauge
with c1:
    st.subheader("Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)")
    if pd.notna(overall_now):
        gauge = go.Figure(go.Indicator(
            mode="gauge+number",
            value=overall_now,
            number={"suffix":" / 100"},
            title={"text": f"Overall Index â€“ {uni} ({current_year})"},
            gauge={
                "axis":{"range":[0,100]},
                "bar":{"thickness":0.2},
                "steps":[
                    {"range":[0,50],"color":"#fce4e4"},
                    {"range":[50,75],"color":"#fff3cd"},
                    {"range":[75,100],"color":"#e8f5e9"},
                ]
            }
        ))
        gauge.update_layout(height=360, margin=dict(l=20,r=20,t=50,b=10))
        st.plotly_chart(gauge, use_container_width=True)
    else:
        st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠÙ…Ø© Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.")

# 3.2 Pie â€“ Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
with c2:
    st.subheader("Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ")
    pie_df = type_breakdown.copy()
    pie_df["function"] = pie_df["function"].map({
        "Leading":"Leading (Ø¥Ù†Ø°Ø§Ø± Ù…Ø¨ÙƒØ±)",
        "Concurrent":"Concurrent (ØªØ´Ø®ÙŠØµ ÙÙˆØ±ÙŠ)",
        "Lagging":"Lagging (Ù‚ÙŠØ§Ø³ Ø£Ø«Ø±)"
    }).fillna(pie_df["function"])
    pie = px.pie(pie_df, names="function", values="w_sum", hole=0.45,
                 title=f"ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†/Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø© â€“ {uni}")
    pie.update_layout(legend_title="", height=360, margin=dict(l=10,r=10,t=60,b=10))
    st.plotly_chart(pie, use_container_width=True)

# 3.3 Trend line
st.markdown("### Ø§Ù„ØªØ±Ù†Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ")
trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year").copy()
if not trend.empty and trend["overall_index_0_100"].notna().any():
    line = px.line(trend, x="year", y="overall_index_0_100", markers=True,
                   title=f"ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€“ {uni}")
    line.update_layout(yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)", xaxis_title="Ø§Ù„Ø³Ù†Ø©")
    st.plotly_chart(line, use_container_width=True)
else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØ±Ù†Ø¯ ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯.")
    line = None  # Ù„Ù„Ø­Ù…Ø§ÙŠØ© Ù„Ø§Ø­Ù‚Ù‹Ø§

# 3.4 Ø¨Ø¯ÙŠÙ„: Ø£Ø¹Ù…Ø¯Ø©
st.markdown("### ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª (Ø£Ø¹Ù…Ø¯Ø©)")
if not trend.empty and trend["overall_index_0_100"].notna().any():
    bar = px.bar(
        trend, x="year", y="overall_index_0_100",
        title=f"ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€“ {uni}", text="overall_index_0_100"
    )
    bar.update_traces(texttemplate="%{text:.1f}", textposition="outside")
    bar.update_layout(
        yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)",
        xaxis_title="Ø§Ù„Ø³Ù†Ø©",
        uniformtext_minsize=10, uniformtext_mode='hide',
        margin=dict(l=10, r=10, t=60, b=10)
    )
    st.plotly_chart(bar, use_container_width=True)
else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©.")

# 3.5 YoY
st.markdown("### Ø§Ù„ØªØºÙŠØ± Ø³Ù†Ø© Ø¨Ø³Ù†Ø© (YoY)")
if len(trend) >= 2 and trend["overall_index_0_100"].notna().any():
    yoy = trend.copy()
    yoy["yoy_delta"] = yoy["overall_index_0_100"].diff()
    yoy = yoy.dropna(subset=["yoy_delta"])
    yoy["direction"] = np.where(yoy["yoy_delta"]>=0, "ØªØ­Ø³Ù†", "ØªØ±Ø§Ø¬Ø¹")
    bar_yoy = px.bar(
        yoy, x="year", y="yoy_delta", color="direction",
        color_discrete_map={"ØªØ­Ø³Ù†":"#2ca02c","ØªØ±Ø§Ø¬Ø¹":"#d62728"},
        title=f"Ø§Ù„ØªØºÙŠØ± Ø³Ù†Ø© Ø¨Ø³Ù†Ø© â€“ {uni}", text="yoy_delta"
    )
    bar_yoy.update_traces(texttemplate="%{text:.1f}", textposition="outside")
    bar_yoy.update_layout(yaxis_title="ÙØ±Ù‚ Ø§Ù„Ù†Ù‚Ø§Ø· Ø¹Ù† Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©", xaxis_title="Ø§Ù„Ø³Ù†Ø©")
    st.plotly_chart(bar_yoy, use_container_width=True)
else:
    st.info("ÙŠÙ„Ø²Ù… ØªÙˆÙØ± Ø³Ù†ØªÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØºÙŠØ± Ø³Ù†Ø© Ø¨Ø³Ù†Ø©.")

# 3.6 Sparkline + Ù…Ù„Ø®Øµ
st.markdown("### Ù„Ù…Ø­Ø© Ø³Ø±ÙŠØ¹Ø© Ø¹Ù† Ø§Ù„Ø§ØªØ¬Ø§Ù‡")
if not trend.empty and trend["overall_index_0_100"].notna().any():
    last = trend.iloc[-1]["overall_index_0_100"]
    prev = trend.iloc[-2]["overall_index_0_100"] if len(trend) >= 2 else np.nan
    delta = (last - prev) if pd.notna(prev) else np.nan

    k1, k2 = st.columns([1,2])
    with k1:
        st.metric("Ø¢Ø®Ø± Ù‚ÙŠÙ…Ø©", f"{last:.1f}", delta=None if pd.isna(delta) else f"{delta:+.1f}")
    with k2:
        spark = px.line(trend, x="year", y="overall_index_0_100")
        spark.update_layout(
            height=140, margin=dict(l=10,r=10,t=10,b=10),
            yaxis_title=None, xaxis_title=None
        )
        spark.update_traces(mode="lines+markers")
        st.plotly_chart(spark, use_container_width=True)

    # ØªÙ…ÙŠÙŠØ² Ø¢Ø®Ø± Ù†Ù‚Ø·Ø© + ÙƒØªØ§Ø¨Ø© ÙØ±ÙˆÙ‚Ø§Øª YoY Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø· (Ø­Ù…Ø§ÙŠØ© ÙˆØ¬ÙˆØ¯ line)
    if line is not None:
        last_row = trend.iloc[-1]
        line.add_scatter(x=[last_row["year"]], y=[last_row["overall_index_0_100"]],
                        mode="markers+text", text=[f"{last_row['overall_index_0_100']:.0f}"],
                        textposition="top center", marker=dict(size=12))
        for i in range(1, len(trend)):
            x_mid = (trend.iloc[i-1]["year"] + trend.iloc[i]["year"]) / 2
            y_mid = (trend.iloc[i-1]["overall_index_0_100"] + trend.iloc[i]["overall_index_0_100"]) / 2
            dlt = trend.iloc[i]["overall_index_0_100"] - trend.iloc[i-1]["overall_index_0_100"]
            line.add_annotation(x=x_mid, y=y_mid, text=f"{dlt:+.0f}", showarrow=False, font=dict(size=12))
else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„Ø®Øµ.")

# ======== 4) ğŸ‡¸ğŸ‡¦ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ÙˆØ·Ù†ÙŠ (Ù…ÙˆØ­Ù‘Ø¯) ========
st.markdown("## ğŸ‡¸ğŸ‡¦ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ÙˆØ·Ù†ÙŠ (Ù…Ø¤Ø´Ø± Ø¬Ø§Ù‡Ø²ÙŠØ© Ù…ÙˆØ­Ù‘Ø¯)")
def build_national_series(overall_df, students_df):
    totals = (students_df.groupby("year", as_index=False)["students"]
              .sum().rename(columns={"students":"national_total"}))
    s = students_df.merge(totals, on="year", how="left")
    s["share"] = np.where(s["national_total"]>0, s["students"]/s["national_total"], 0.0)
    o = overall_df.rename(columns={"overall_index_0_100":"uni_index"})
    merged_ns = o.merge(s[["university","year","share"]], on=["university","year"], how="left")
    merged_ns["weighted"] = merged_ns["uni_index"] * merged_ns["share"]
    nat = (merged_ns.groupby("year", as_index=False)["weighted"].sum()
           .rename(columns={"weighted":"national_index"}))
    nat = nat.sort_values("year").reset_index(drop=True)
    return nat

nat = build_national_series(overall, students_df)

left_nat, right_nat = st.columns([2,1])
with left_nat:
    if not nat.empty and nat["national_index"].notna().any():
        fig_nat = px.line(nat, x="year", y="national_index", markers=True,
                          title="Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© (0â€“100)")
        fig_nat.update_layout(yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙˆØ·Ù†ÙŠ", xaxis_title="Ø§Ù„Ø³Ù†Ø©")
        st.plotly_chart(fig_nat, use_container_width=True)
    else:
        st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ©.")

with right_nat:
    horizon = st.slider("Ø£ÙÙ‚ Ø§Ù„ØªÙ†Ø¨Ø¤ (Ø¨Ø§Ù„Ø³Ù†ÙˆØ§Øª)", 1, 5, 3)

def forecast_national(nat_df, horizon):
    y = nat_df.set_index("year")["national_index"].astype(float)
    if len(y) < 3:
        xs = np.arange(len(y))
        if len(xs) == 0:
            return nat_df.copy(), pd.DataFrame(columns=["year","forecast"])
        A = np.vstack([xs, np.ones(len(xs))]).T
        m, c = np.linalg.lstsq(A, y.values, rcond=None)[0]
        last_year = int(y.index.max())
        fut_years = [last_year + i for i in range(1, horizon+1)]
        fut_vals = [m*(len(xs)+i-1) + c for i in range(1, horizon+1)]
        fc = pd.DataFrame({"year": fut_years, "forecast": fut_vals})
        return nat_df.copy(), fc
    model = ExponentialSmoothing(y, trend='add', seasonal=None, initialization_method="estimated")
    fit = model.fit(optimized=True)
    last_year = int(y.index.max())
    fut_index = [last_year + i for i in range(1, horizon+1)]
    fc_values = fit.forecast(horizon)
    fc = pd.DataFrame({"year": fut_index, "forecast": fc_values.values})
    return nat_df.copy(), fc

if not nat.empty and nat["national_index"].notna().any():
    hist, fc = forecast_national(nat, horizon)
    nat_fc = hist.merge(fc, on="year", how="outer").sort_values("year")
    fig_fc = go.Figure()
    hist_part = nat_fc[nat_fc["national_index"].notna()]
    fig_fc.add_trace(go.Scatter(x=hist_part["year"], y=hist_part["national_index"],
                                mode="lines+markers", name="ØªØ§Ø±ÙŠØ®"))
    fc_part = nat_fc[nat_fc["forecast"].notna()]
    fig_fc.add_trace(go.Scatter(x=fc_part["year"], y=fc_part["forecast"],
                                mode="lines+markers", name="ØªÙ†Ø¨Ø¤", line=dict(dash="dash")))
    fig_fc.update_layout(title=f"ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙˆØ·Ù†ÙŠ Ø­ØªÙ‰ {int(fc_part['year'].max()) if not fc_part.empty else ''}",
                         yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙˆØ·Ù†ÙŠ (0â€“100)", xaxis_title="Ø§Ù„Ø³Ù†Ø©",
                         margin=dict(l=10,r=10,t=60,b=10))
    st.plotly_chart(fig_fc, use_container_width=True)

    if not fc.empty:
        st.success(
            f"**Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ø³Ù†Ø© {int(fc.iloc[0]['year'])}: {fc.iloc[0]['forecast']:.1f}**  â€¢ "
            f"**Ø£Ø¹Ù„Ù‰ Ø³Ù†Ø© Ù…ØªÙˆÙ‚Ø¹Ø© Ø¶Ù…Ù† Ø§Ù„Ø£ÙÙ‚: {int(fc.loc[fc['forecast'].idxmax(),'year'])} ({fc['forecast'].max():.1f})**"
        )
else:
    st.info("Ø£Ø¶Ù/ØµØ­Ù‘Ø­ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù„Ø§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ÙˆØ·Ù†ÙŠ.")

# ======== 5) Ø´Ø±Ø­ Ù…Ø®ØªØµØ± Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø·Ù„Ø§Ø¨ ========
with st.expander("Ù…Ø§Ø°Ø§ ÙŠØ¶ÙŠÙ Ù‚Ø³Ù… Ø§Ù„Ø·Ù„Ø§Ø¨ØŸ"):
    st.markdown("""
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨**: ÙŠØ¹Ø·ÙŠ Ø­Ø¬Ù… Ø§Ù„Ø¹ÙŠÙ†Ø© ÙÙŠ ÙØªØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø®ØªØ§Ø±).
- **Ø·Ù„Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©**: ÙŠÙÙŠØ¯ ÙÙŠ ØªÙØ³ÙŠØ± ØªØºÙŠÙ‘Ø±Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø± (Ø§Ø±ØªÙØ§Ø¹/Ø§Ù†Ø®ÙØ§Ø¶ Ù‚Ø¯ ÙŠØ±ØªØ¨Ø· Ø¨Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©).
- **Ø­ØµØ© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù…Ù† Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ**: ØªÙˆØ¶Ø­ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ù†Ø³Ø¨ÙŠ ÙˆØ§Ù„ÙˆØ²Ù† Ø§Ù„ÙˆØ·Ù†ÙŠ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†Ø©.
- **Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠ**: Ù…Ù‚Ø§Ø±Ù†Ø© Ø³Ø±ÙŠØ¹Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª ÙÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ Ù„Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.
""")

# ===============================
# 6) (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) ØªÙƒØ§Ù…Ù„ Ø³Ø±ÙŠØ¹ Ù…Ø¹ Streamlit
# ===============================
# Ù…Ø«Ø§Ù„ Ø¨Ø·Ø§Ù‚Ø§Øª KPI Ù„Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
try:
    import streamlit as st
    import plotly.express as px

    st.markdown("## ğŸ§­ Ø®Ù„Ø§ØµØ© ØªÙ†ÙÙŠØ°ÙŠØ©")
    universities = sorted(overall["university"].unique().tolist())
    uni = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©", universities, index=0)

    st.write(make_executive_brief(uni))

    # Ù…ØµÙÙˆÙØ© Ø­Ø±Ø§Ø±ÙŠØ© 3Ã—5 (ÙˆØ¸Ø§Ø¦Ù Ã— Ø§ØªØ¬Ø§Ù‡Ø§Øª) Ø¨Ø­Ø³Ø¨ Ø¢Ø®Ø± Ø³Ù†Ø©
    trends_order = ["High Upper Trend","Moderate Upper Trend","Steady","Moderate Lower Trend","High Lower Trend"]
    func_order = ["Leading","Concurrent","Lagging"]
    grid = (func_insights_df[func_insights_df["university"]==uni]
            .groupby(["function","trend5"], as_index=False).size())
    # ØªØ¬Ù‡ÙŠØ² pivot
    heat = (grid
            .pivot(index="function", columns="trend5", values="size")
            .reindex(index=func_order, columns=trends_order).fillna(0))
    st.markdown("### ğŸ”³ Ù…ØµÙÙˆÙØ© (Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ã— Ø§Ù„ÙˆØ¸ÙŠÙØ©)")
    st.dataframe(heat.style.background_gradient(axis=None))

    # Drill-down: Ø£ÙØ¶Ù„ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù„ÙƒÙ„ Ù‡Ø¯Ù
    st.markdown("### ğŸ” ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù")
    goals_ = sorted(goal_level["goal"].unique().tolist())
    gsel = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù", goals_, index=0)
    subg = func_goal_insights_df[(func_goal_insights_df["university"]==uni) & (func_goal_insights_df["goal"]==gsel)]
    st.dataframe(subg.sort_values("priority_score", ascending=False)[
        ["function","trend5","latest_year","latest_score","priority","interpretation_ar","action_ar"]
    ])
except Exception as _e:
    # Ø¨ÙŠØ¦Ø© ØºÙŠØ± Ø¯Ø§Ø¹Ù…Ø© Ù„Ù€ Streamlit/PlotlyØŸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
    pass









# # -----------------------------
# # 5) ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
# # -----------------------------


# # -----------------------------
# # 6)  Ø§Ù„Ø¯Ø§Ù„Ù‡() ØªØ³ØªØ®Ø¯Ù… Ø¹Ø§Ù…ÙˆØ¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠÙ‡  (Ø§Ù„ØªØ·Ø¨ÙŠØ¹) ÙˆÙ‚Ø¯ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ Ø¹Ù…ÙˆØ¯ () Ø­ÙŠØ«
# # -----------------------------

# def aggregate(df, by_cols):
#     grouped = []
#     for keys, sub in df.groupby(by_cols):
#         # 1)ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ÙˆØ²Ø§Ù† ÙˆØ§Ù„Ù‚ÙŠÙ…
#         w = sub["weight"] if "weight" in sub else pd.Series([np.nan]*len(sub))
#         v = sub["norm_0_100"].values

#         # 2)
#         den = w.sum()
#         if den == 0:
#             raise ValueError(f"Ø§Ù„Ø£ÙˆØ²Ø§Ù† = 0 ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© {keys} â†’ Ø´ÙŠ ØºÙ„Ø· Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

#         val = (v @ w) / den  # Ø­Ø§ØµÙ„ Ø¶Ø±Ø¨ Ø¯Ø§Ø®Ù„ÙŠ Ã· Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†

#         # 3) Ø¨Ù†Ø§Ø¡ Ø³Ø¬Ù„ Ø§Ù„Ù†Ø§ØªØ¬ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
#         rec = dict(zip(by_cols, keys if isinstance(keys, tuple) else (keys,)))
#         rec["score"] = val
#         grouped.append(rec)
#     return pd.DataFrame(grouped)

# # # -----------------------------
# # # 7)  Ù‡Ù†Ø§ Ù†Ø±ÙŠØ¯ Ø­Ø³Ø§Ø¨ Ø¯Ø§Ù„Ù‡ ØªØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ù‡ * Ø§Ù„Ø³Ù†Ù‡ * Ø§Ù„Ù‡Ø¯Ù * Ø§Ù„ÙˆØ¸ÙŠÙÙ‡  ÙˆÙ‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„ ØªØ­Ø³Ø¨ Ù…ØªÙˆØ³Ø· Ù…ÙˆØ²ÙˆÙ†Ø§ Ø¹Ø¨Ø± Ø§Ù„Ù…ÙˆØ´Ø±Ø§Øª Ù„Ø§Ù† ( ) ÙŠØ­Ù…Ù„ ÙˆØ²Ù† ÙƒÙ„ Ù…ÙˆØ´Ø±
# # # -----------------------------

# # func_level = aggregate(merged, ["university","year","goal","function"])

# # # -----------------------------
# # # 8) Ù‡Ù†Ø§ ÙÙŠ Ù…Ø³ØªÙˆÙŠ Ø§Ù„Ù‡Ø¯Ù Ø¹Ù†Ø¯Ù†Ø§ Ø¯Ø§ØªØ§ ÙˆØ¸ÙŠÙÙŠÙ‡ Ø¨Ù„Ø§ Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙˆØ²Ù† Ø¨Ø¹Ø¯ Ø§Ø¹Ø§Ø¯Ù‡ ØªØ³Ù…ÙŠØªÙ‡ Ø¨ score  Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§ÙˆØ²Ø§Ù† Ù‡Ù†Ø§ ØªØ³ØªØ®Ø¯Ù… Ù…ØªÙˆØ³Ø· Ø¨Ø³ÙŠØ· Ø¨ÙŠÙ† Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø«Ù„Ø§Ø«Ù‡ lagging + concurrent + leeading

# # # -----------------------------
# # goal_level = aggregate(func_level.rename(columns={"score":"norm_0_100"}), ["university","year","goal"])

# # overall = aggregate(goal_level.rename(columns={"score":"norm_0_100"}), ["university","year"]).rename(columns={"score":"overall_index_0_100"})

# # -----------------------------
# # 7)
# # -----------------------------
# # 1) Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‡Ø¯Ù (weighted by indicator weights)
# func_level = (
#     merged
#     .assign(wv = merged["norm_0_100"] * merged["weight"])
#     .groupby(["university","year","goal","function"], as_index=False)
#     .agg(score_sum=("wv","sum"), wsum=("weight","sum"))
#     .assign(score = lambda d: np.where(d["wsum"]>0, d["score_sum"]/d["wsum"], np.nan))
#     .drop(columns=["score_sum"])
# )

# # 2) Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù‡Ø¯Ù (weighted by total weights carried from functions)
# goal_level = (
#     func_level
#     .assign(wv = func_level["score"] * func_level["wsum"])
#     .groupby(["university","year","goal"], as_index=False)
#     .agg(score_sum=("wv","sum"), wsum=("wsum","sum"))
#     .assign(score = lambda d: np.where(d["wsum"]>0, d["score_sum"]/d["wsum"], np.nan))
#     .drop(columns=["score_sum"])
# )

# # 3) Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ (weighted by total weights carried from goals)
# overall = (
#     goal_level
#     .assign(wv = goal_level["score"] * goal_level["wsum"])
#     .groupby(["university","year"], as_index=False)
#     .agg(score_sum=("wv","sum"), wsum=("wsum","sum"))
#     .assign(overall_index_0_100 = lambda d: np.where(d["wsum"]>0, d["score_sum"]/d["wsum"], np.nan))
#     .drop(columns=["score_sum","wsum"])
# )


# # -----------------------------
# # 8) Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù€ Leading
# # -----------------------------
# lead = func_level[func_level["function"]=="Leading"]
# signals=[]
# for (uni,goal),sub in lead.groupby(["university","goal"]):
#     s = slope({int(r["year"]):float(r["score"]) for _,r in sub.iterrows()})
#     signals.append({"university":uni,"goal":goal,"slope":s,"trend":classify_trend(s),
#                     "signal":"Early Signal" if s>=2 else ("Early Warning" if s<=-2 else "Neutral")})
# signals_df = pd.DataFrame(signals)



# # -----------------------------
# # 7) Ø±Ø³ÙˆÙ…
# # -----------------------------
# latest=max(years)
# bar=goal_level[goal_level["year"]==latest].pivot(index="goal",columns="university",values="score")
# bar.plot(kind="bar",title=f"Goal Scores {latest}"); plt.ylabel("0â€“100"); plt.show()

# for uni in universities:
#     mat=goal_level[goal_level["university"]==uni].pivot(index="goal",columns="year",values="score")
#     plt.imshow(mat.values,aspect="auto"); plt.colorbar(label="0â€“100")
#     plt.title(f"Heatmap - {uni}"); plt.xticks(range(len(mat.columns)),mat.columns); plt.yticks(range(len(mat.index)),mat.index)
#     plt.show()

# # --------------------------
# # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„ÙƒÙ„ Ù†ÙˆØ¹
# # --------------------------
# np.random.seed(42)
# if 'value' not in indicators_meta2.columns:
#     indicators_meta2['value'] = np.random.rand(len(indicators_meta2))

# indices_df = indicators_meta2.groupby('function').apply(
#     lambda x: (x['value'] * x['weight']).sum()
# ).reset_index(name='Index')

# # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
# overall_index = indices_df['Index'].sum()
# indices_df = pd.concat([indices_df, pd.DataFrame([{'function': 'Overall', 'Index': overall_index}])], ignore_index=True)

# print(indices_df)

# plt.figure(figsize=(10,6))
# colors = {'Leading':'#FFA500', 'Concurrent':'#00BFFF', 'Lagging':'#32CD32', 'Overall':'#FF69B4'}
# plt.bar(indices_df['function'], indices_df['Index'], color=[colors[t] for t in indices_df['function']])
# plt.title("Overall Index of Graduate Readiness")
# plt.ylabel("Weighted Index")
# plt.ylim(0, 1)  # Ø¥Ø°Ø§ Ø§Ù„Ù‚ÙŠÙ… Ø¨ÙŠÙ† 0 Ùˆ 1
# plt.show()

# st.set_page_config(page_title="Overall Graduate Readiness Index", layout="wide")
# st.title("ğŸ“ˆ Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ†")
# st.caption("Gauge Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€¢ Pie Ù„Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ â€¢ Trend Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª â€¢ ğŸ‘¨â€ğŸ“ Ø¹Ø¯Ù‘Ø§Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨")

# # =========================
# # 0) Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© + Ø¨ÙŠØ§Ù†Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
# # =========================
# def likert_to_100(x):
#     return ((x - 1.0) / 4.0) * 100.0  # 1..5 -> 0..100

# def minmax_0_100(series, higher_is_better=True):
#     s = series.astype(float)
#     mn, mx = s.min(), s.max()
#     scaled = (s - mn) / (mx - mn) * 100.0 if mx > mn else pd.Series(50.0, index=s.index)
#     return scaled if higher_is_better else 100.0 - scaled

# def weighted_mean(values, weights):
#     values = np.asarray(values, dtype=float)
#     weights = np.asarray(weights, dtype=float)
#     if np.all(np.isnan(weights)) or np.nansum(weights) == 0:
#         return float(np.nanmean(values))
#     return float(np.nansum(values * weights) / np.nansum(weights))

# def build_demo():
#     rng = np.random.default_rng(42)
#     # 30 Ù…Ø¤Ø´Ø±: 8 Leading / 7 Concurrent / 15 Lagging
#     kinds = (["Leading"]*8) + (["Concurrent"]*7) + (["Lagging"]*15)
#     weights = rng.uniform(0.5, 2.0, size=30); weights = weights / weights.sum()
#     rows = []
#     for i in range(30):
#         rows.append({
#             "indicator_id": f"IND_{i+1:02d}",
#             "name": f"Indicator {i+1}",
#             "component_details": "",
#             "component": "",
#             "weight": weights[i],
#             "function": kinds[i],
#             "goal": rng.integers(1, 5),
#             "direction": "HigherIsBetter",
#             "is_likert": bool(rng.random() < 0.25),
#         })
#     indicators_meta2 = pd.DataFrame(rows)

#     universities = ["UniA", "UniB", "UniC"]
#     years = [2021, 2022, 2023, 2024]
#     raw = []
#     for u in universities:
#         for y in years:
#             for _, r in indicators_meta2.iterrows():
#                 val = rng.uniform(2.5, 4.7) if r["is_likert"] else rng.uniform(40, 95)
#                 raw.append((u, y, r["indicator_id"], val))
#     raw_df = pd.DataFrame(raw, columns=["university","year","indicator_id","raw_value"])

#     # ØªØ·Ø¨ÙŠØ¹ 0â€“100 Ù„ÙƒÙ„ Ù…Ø¤Ø´Ø±
#     norm_blocks = []
#     for ind, sub in raw_df.groupby("indicator_id"):
#         meta = indicators_meta2.loc[indicators_meta2["indicator_id"]==ind].iloc[0]
#         sub = sub.copy()
#         if meta["is_likert"]:
#             sub["norm_0_100"] = likert_to_100(sub["raw_value"])
#         else:
#             sub["norm_0_100"] = minmax_0_100(sub["raw_value"], higher_is_better=(meta["direction"]!="LowerIsBetter"))
#         norm_blocks.append(sub)
#     norm_df = pd.concat(norm_blocks, ignore_index=True)

#     merged = norm_df.merge(indicators_meta2, on="indicator_id", how="left")

#     def aggregate(df, by_cols):
#         grouped = []
#         for keys, s in df.groupby(by_cols):
#             w = s["weight"] if "weight" in s else pd.Series([np.nan]*len(s))
#             v = s["norm_0_100"].values
#             val = weighted_mean(v, w)
#             rec = dict(zip(by_cols, keys if isinstance(keys, tuple) else (keys,)))
#             rec["score"] = val
#             grouped.append(rec)
#         return pd.DataFrame(grouped)

#     func_level = aggregate(merged, ["university","year","goal","function"])
#     goal_level = aggregate(func_level.rename(columns={"score":"norm_0_100"}), ["university","year","goal"])
#     overall = aggregate(goal_level.rename(columns={"score":"norm_0_100"}), ["university","year"]).rename(columns={"score":"overall_index_0_100"})

#     # Ø·Ù„Ø§Ø¨ ØªØ¬Ø±ÙŠØ¨ÙŠ: ØªÙˆØ²ÙŠØ¹ Ø¹Ø´ÙˆØ§Ø¦ÙŠ ÙˆØ§Ù‚Ø¹ÙŠ
#     srows = []
#     for y in years:
#         # Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆØ·Ù†ÙŠ ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„ÙƒÙ„ Ø³Ù†Ø©
#         national_total = int(rng.integers(24000, 36000))
#         # Ù†Ø³Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª
#         shares = rng.random(3); shares = shares / shares.sum()
#         for u, sh in zip(universities, shares):
#             srows.append((u, y, int(national_total * sh)))
#     students_df = pd.DataFrame(srows, columns=["university","year","students"])

#     return indicators_meta2, raw_df, func_level, goal_level, overall, students_df

# g = globals()
# need_demo = False
# try:
#     indicators_meta2 = g["indicators_meta2"]
#     raw_df = g["raw_df"]
#     func_level = g["func_level"]
#     goal_level = g["goal_level"]
#     overall = g["overall"]
# except KeyError:
#     need_demo = True

# if need_demo:
#     indicators_meta2, raw_df, func_level, goal_level, overall, students_df = build_demo()
# else:
#     # Ù„Ùˆ Ø¹Ù†Ø¯Ù†Ø§ Ù…Ù„Ù Ø·Ù„Ø§Ø¨ Ù†Ù‚Ø¯Ø± Ù†Ù‚Ø±Ø£Ù‡ Ù…Ù† Ù‡Ù†Ø§:
#     students_file = st.sidebar.file_uploader("ğŸ“¥ Ø­Ù…Ù‘Ù„ Ù…Ù„Ù Ø§Ù„Ø·Ù„Ø§Ø¨ (students.csv)", type=["csv"])
#     if students_file is not None:
#         students_df = pd.read_csv(students_file)
#     else:
#         # ÙÙŠ Ø­Ø§Ù„ Ù…Ø§ Ø¹Ù†Ø¯Ù†Ø§ Ù…Ù„Ù Ø²ÙŠ Ø­Ø§Ù„ØªÙ†Ø§ Ø§Ù„Ø­ÙŠÙ†ØŒ Ù†ØµÙ†Ø¹ Ø¯Ø§ØªØ§ Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ù† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª/Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
#         _, _, _, _, _, students_df = build_demo()

# # =========================
# # 1) Ø§Ù„Ù…Ø±Ø´Ù‘Ø­Ø§Øª
# # =========================
# left, right = st.columns([1,1])
# with left:
#     universities = sorted(overall["university"].unique().tolist())
#     uni = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©", universities, index=0)
# with right:
#     years = sorted(overall["year"].unique().tolist())
#     year_start, year_end = st.select_slider("Ù†Ø·Ø§Ù‚ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ù„Ù„ØªØ±Ù†Ø¯", options=years, value=(years[0], years[-1]))

# current_year = year_end

# # Ø§Ø·Ø§Ø±Ø§Øª Ù…Ø±Ø´Ù‘Ø­Ø©
# overall_f = overall[(overall["university"]==uni)]
# func_f = func_level[(func_level["university"]==uni)]

# # Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ (Ø­Ø³Ø¨ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†)
# type_weights = indicators_meta2.groupby("function", as_index=False)["weight"].sum().rename(columns={"weight":"w_sum"})
# type_score_now = func_f[func_f["year"]==current_year].groupby("function", as_index=False)["score"].mean()
# type_breakdown = type_weights.merge(type_score_now, on="function", how="left").fillna({"score":0})

# # Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
# overall_now = overall_f[overall_f["year"]==current_year]["overall_index_0_100"]
# overall_now = float(overall_now.iloc[0]) if len(overall_now) else np.nan

# # =========================
# # 2) ğŸ‘¨â€ğŸ“ Ù‚Ø³Ù… Ø¹Ø¯Ù‘Ø§Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨
# # =========================
# st.markdown("### ğŸ‘¨â€ğŸ“ Ø§Ù„Ø·Ù„Ø§Ø¨")
# # ØªØµÙÙŠØ© Ø§Ù„Ø·Ù„Ø§Ø¨ Ø­Ø³Ø¨ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø³Ù†ÙˆØ§Øª
# students_in_range = students_df[(students_df["year"]>=year_start) & (students_df["year"]<=year_end)]
# # Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ (ÙˆØ·Ù†ÙŠ) Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ø·Ø§Ù‚
# national_total_range = int(students_in_range["students"].sum())
# # Ø·Ù„Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ø®ØªØ§Ø±Ø© ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
# uni_current_students = int(
#     students_df[(students_df["university"]==uni) & (students_df["year"]==current_year)]["students"].sum()
# ) if not students_df.empty else 0

# m1, m2, m3 = st.columns(3)
# with m1:
#     st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ (ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø®ØªØ§Ø±)", f"{national_total_range:,}")
# with m2:
#     st.metric(f"Ø·Ù„Ø§Ø¨ {uni} ÙÙŠ {current_year}", f"{uni_current_students:,}")
# with m3:
#     # Ø­ØµØ© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù…Ù† Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†Ø©
#     national_current_total = int(students_df[students_df["year"]==current_year]["students"].sum())
#     share = (uni_current_students / national_current_total * 100.0) if national_current_total>0 else 0.0
#     st.metric("Ø­ØµØ© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ù†Ø©", f"{share:.1f}%")

# # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
# st.write(f"**ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© â€“ {current_year}**")
# students_current_year = students_df[students_df["year"]==current_year].copy()
# if not students_current_year.empty:
#     bar_students = px.bar(students_current_year, x="university", y="students", text="students",
#                           title=f"Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ({current_year})")
#     bar_students.update_layout(yaxis_title="Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨", xaxis_title="Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©")
#     bar_students.update_traces(texttemplate="%{text:,}", textposition="outside")
#     st.plotly_chart(bar_students, use_container_width=True)
# else:
#     st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø·Ù„Ø§Ø¨ Ù„Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.")

# # =========================
# # 3) Visualization Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
# # =========================
# c1, c2 = st.columns([1,1])

# # 3.1 Gauge (Speedometer) Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
# with c1:
#     st.subheader("Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)")
#     if pd.notna(overall_now):
#         gauge = go.Figure(go.Indicator(
#             mode="gauge+number",
#             value=overall_now,
#             number={"suffix":" / 100"},
#             title={"text": f"Overall Index â€“ {uni} ({current_year})"},
#             gauge={
#                 "axis":{"range":[0,100]},
#                 "bar":{"thickness":0.2},
#                 "steps":[
#                     {"range":[0,50],"color":"#fce4e4"},
#                     {"range":[50,75],"color":"#fff3cd"},
#                     {"range":[75,100],"color":"#e8f5e9"},
#                 ]
#             }
#         ))
#         gauge.update_layout(height=360, margin=dict(l=20,r=20,t=50,b=10))
#         st.plotly_chart(gauge, use_container_width=True)
#     else:
#         st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠÙ…Ø© Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.")

# # 3.2 Pie â€“ Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙ„ÙŠ (Ø­Ø³Ø¨ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†)
# with c2:
#     st.subheader("Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ")
#     pie_df = type_breakdown.copy()
#     pie_df["function"] = pie_df["function"].map({
#         "Leading":"Leading (Ø¥Ù†Ø°Ø§Ø± Ù…Ø¨ÙƒØ±)",
#         "Concurrent":"Concurrent (ØªØ´Ø®ÙŠØµ ÙÙˆØ±ÙŠ)",
#         "Lagging":"Lagging (Ù‚ÙŠØ§Ø³ Ø£Ø«Ø±)"
#     }).fillna(pie_df["function"])

#     pie = px.pie(pie_df, names="function", values="w_sum", hole=0.45,
#                  title=f"ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹ â€“ {uni}")
#     pie.update_layout(legend_title="", height=360, margin=dict(l=10,r=10,t=60,b=10))
#     st.plotly_chart(pie, use_container_width=True)

# # 3.3 Trend line â€“ ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª
# st.markdown("### Ø§Ù„ØªØ±Ù†Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ")
# trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year")
# if not trend.empty and trend["overall_index_0_100"].notna().any():
#     line = px.line(trend, x="year", y="overall_index_0_100", markers=True,
#                    title=f"ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€“ {uni}")
#     line.update_layout(yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)", xaxis_title="Ø§Ù„Ø³Ù†Ø©")
#     st.plotly_chart(line, use_container_width=True)
# else:
#     st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØ±Ù†Ø¯ ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯.")


# # 3.4 Ø¨Ø¯ÙŠÙ„: Ø¹Ù…ÙˆØ¯ÙŠØ§Øª Ù…Ø¹ ØªØ³Ù…ÙŠØ§Øª
# st.markdown("### ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª")
# trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year")
# if not trend.empty and trend["overall_index_0_100"].notna().any():
#     bar = px.bar(
#         trend, x="year", y="overall_index_0_100",
#         title=f"ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€“ {uni}", text="overall_index_0_100"
#     )
#     bar.update_traces(texttemplate="%{text:.1f}", textposition="outside")
#     bar.update_layout(
#         yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)",
#         xaxis_title="Ø§Ù„Ø³Ù†Ø©",
#         uniformtext_minsize=10, uniformtext_mode='hide',
#         margin=dict(l=10, r=10, t=60, b=10)
#     )
#     st.plotly_chart(bar, use_container_width=True)
# else:
#     st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØ±Ù†Ø¯ ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯.")

# # 3.5 Ø¨Ø¯ÙŠÙ„: ÙØ±ÙˆÙ‚Ø§Øª Ø³Ù†Ø© Ø¨Ø³Ù†Ø©
# st.markdown("### Ø§Ù„ØªØºÙŠØ± Ø³Ù†Ø© Ø¨Ø³Ù†Ø© (YoY)")
# trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year")
# if len(trend) >= 2 and trend["overall_index_0_100"].notna().any():
#     trend["yoy_delta"] = trend["overall_index_0_100"].diff()
#     yoy = trend.dropna(subset=["yoy_delta"])
#     # ØªÙ„ÙˆÙŠÙ† Ø­Ø³Ø¨ Ù…ÙˆØ¬Ø¨/Ø³Ø§Ù„Ø¨
#     yoy["direction"] = np.where(yoy["yoy_delta"]>=0, "ØªØ­Ø³Ù†", "ØªØ±Ø§Ø¬Ø¹")
#     bar_yoy = px.bar(
#         yoy, x="year", y="yoy_delta", color="direction",
#         color_discrete_map={"ØªØ­Ø³Ù†":"#2ca02c","ØªØ±Ø§Ø¬Ø¹":"#d62728"},
#         title=f"Ø§Ù„ØªØºÙŠØ± Ø³Ù†Ø© Ø¨Ø³Ù†Ø© â€“ {uni}", text="yoy_delta"
#     )
#     bar_yoy.update_traces(texttemplate="%{text:.1f}", textposition="outside")
#     bar_yoy.update_layout(yaxis_title="ÙØ±Ù‚ Ø§Ù„Ù†Ù‚Ø§Ø· Ø¹Ù† Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©", xaxis_title="Ø§Ù„Ø³Ù†Ø©")
#     st.plotly_chart(bar_yoy, use_container_width=True)
# else:
#     st.info("ÙŠÙ„Ø²Ù… ØªÙˆÙØ± Ø³Ù†ØªÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØºÙŠØ± Ø³Ù†Ø© Ø¨Ø³Ù†Ø©.")

# # 3.6 Ø¨Ø¯ÙŠÙ„: Sparkline + Ù…Ù„Ø®Øµ
# st.markdown("### Ù„Ù…Ø­Ø© Ø³Ø±ÙŠØ¹Ø© Ø¹Ù† Ø§Ù„Ø§ØªØ¬Ø§Ù‡")
# trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year")
# if not trend.empty and trend["overall_index_0_100"].notna().any():
#     last = trend.iloc[-1]["overall_index_0_100"]
#     prev = trend.iloc[-2]["overall_index_0_100"] if len(trend) >= 2 else np.nan
#     delta = (last - prev) if pd.notna(prev) else np.nan

#     k1, k2 = st.columns([1,2])
#     with k1:
#         st.metric("Ø¢Ø®Ø± Ù‚ÙŠÙ…Ø©", f"{last:.1f}", delta=None if pd.isna(delta) else f"{delta:+.1f}")
#     with k2:
#         spark = px.line(trend, x="year", y="overall_index_0_100")
#         spark.update_layout(
#             height=140, margin=dict(l=10,r=10,t=10,b=10),
#             yaxis_title=None, xaxis_title=None
#         )
#         spark.update_traces(mode="lines+markers")
#         st.plotly_chart(spark, use_container_width=True)
# else:
#     st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„Ø®Øµ.")

# # ØªÙ…ÙŠÙŠØ² Ø¢Ø®Ø± Ù†Ù‚Ø·Ø©
# last_row = trend.iloc[-1]
# line.add_scatter(x=[last_row["year"]], y=[last_row["overall_index_0_100"]],
#                  mode="markers+text", text=[f"{last_row['overall_index_0_100']:.0f}"],
#                  textposition="top center", marker=dict(size=12))

# # ÙƒØªØ§Ø¨Ø© ÙØ±ÙˆÙ‚Ø§Øª Ø³Ù†Ø© Ø¨Ø³Ù†Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø·
# for i in range(1, len(trend)):
#     x_mid = (trend.iloc[i-1]["year"] + trend.iloc[i]["year"]) / 2
#     y_mid = (trend.iloc[i-1]["overall_index_0_100"] + trend.iloc[i]["overall_index_0_100"]) / 2
#     delta = trend.iloc[i]["overall_index_0_100"] - trend.iloc[i-1]["overall_index_0_100"]
#     line.add_annotation(x=x_mid, y=y_mid, text=f"{delta:+.0f}", showarrow=False, font=dict(size=12))

# # =========================
# # 3.7 Ø­Ø±Ø§Ø³Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… 'line' Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¥Ù† ÙƒØ§Ù†Øª Ù…Ø¹Ø±ÙØ©
# # =========================
# if 'line' not in locals():
#     line = None

# # =========================
# # 3.8 ğŸ‡¸ğŸ‡¦ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ÙˆØ·Ù†ÙŠ (Ù…ÙˆØ­Ù‘Ø¯)
# # =========================
# st.markdown("## ğŸ‡¸ğŸ‡¦ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ÙˆØ·Ù†ÙŠ (Ù…Ø¤Ø´Ø± Ø¬Ø§Ù‡Ø²ÙŠØ© Ù…ÙˆØ­Ù‘Ø¯)")
# from statsmodels.tsa.holtwinters import ExponentialSmoothing

# # 1) Ù†Ø¨Ù†ÙŠ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© (ÙˆØ²Ù† ÙƒÙ„ Ø¬Ø§Ù…Ø¹Ø© Ø¨Ø­ØµØªÙ‡Ø§ Ù…Ù† Ø§Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†Ø©)
# def build_national_series(overall_df, students_df):
#     # Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ Ù„ÙƒÙ„ Ø³Ù†Ø©
#     totals = students_df.groupby("year", as_index=False)["students"].sum().rename(columns={"students":"national_total"})
#     # Ø¶Ù… Ø­ØµØ© ÙƒÙ„ Ø¬Ø§Ù…Ø¹Ø© Ø¨Ø§Ù„Ø³Ù†Ø©
#     s = students_df.merge(totals, on="year", how="left")
#     s["share"] = np.where(s["national_total"]>0, s["students"]/s["national_total"], 0.0)
#     # Ø§Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„ÙƒÙ„ Ø¬Ø§Ù…Ø¹Ø©/Ø³Ù†Ø©
#     o = overall_df.rename(columns={"overall_index_0_100":"uni_index"})
#     merged = o.merge(s[["university","year","share"]], on=["university","year"], how="left")
#     merged["weighted"] = merged["uni_index"] * merged["share"]
#     nat = merged.groupby("year", as_index=False)["weighted"].sum().rename(columns={"weighted":"national_index"})
#     # ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„Ù†Ø¹ÙˆÙ…Ø© Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
#     nat = nat.sort_values("year").reset_index(drop=True)
#     return nat

# nat = build_national_series(overall, students_df)

# left_nat, right_nat = st.columns([2,1])
# with left_nat:
#     if not nat.empty and nat["national_index"].notna().any():
#         fig_nat = px.line(nat, x="year", y="national_index", markers=True,
#                           title="Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© (0â€“100)")
#         fig_nat.update_layout(yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙˆØ·Ù†ÙŠ", xaxis_title="Ø§Ù„Ø³Ù†Ø©")
#         st.plotly_chart(fig_nat, use_container_width=True)
#     else:
#         st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ©.")

# with right_nat:
#     horizon = st.slider("Ø£ÙÙ‚ Ø§Ù„ØªÙ†Ø¨Ø¤ (Ø¨Ø§Ù„Ø³Ù†ÙˆØ§Øª)", 1, 5, 3)

# # 2) Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ (Holt-Winters Ø¨Ø¯ÙˆÙ† Ù…ÙˆØ³Ù…ÙŠØ© â€” Ø³Ù†ÙˆÙŠ)

# st.markdown("### Ø§Ù„ØªÙ†Ø¨Ø¤ 1")

# def forecast_national(nat_df, horizon):
#     y = nat_df.set_index("year")["national_index"].astype(float)
#     # Ø¥Ù† ÙƒØ§Ù†Øª Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…ÙŠÙ„ Ø®Ø·ÙŠ Ø¨Ø³ÙŠØ· ÙƒØ¨Ø¯ÙŠÙ„
#     if len(y) < 3:
#         # Ø§Ù†Ø­Ø¯Ø§Ø± Ø®Ø·ÙŠ Ø¨Ø³ÙŠØ·
#         xs = np.arange(len(y))
#         A = np.vstack([xs, np.ones(len(xs))]).T
#         m, c = np.linalg.lstsq(A, y.values, rcond=None)[0]
#         last_year = int(y.index.max())
#         hist = nat_df.copy()
#         fut_years = [last_year + i for i in range(1, horizon+1)]
#         fut_vals = [m*(len(xs)+i-1) + c for i in range(1, horizon+1)]
#         fc = pd.DataFrame({"year": fut_years, "forecast": fut_vals})
#         return hist, fc
#     # Holt-Winters (trend additive)
#     model = ExponentialSmoothing(y, trend='add', seasonal=None, initialization_method="estimated")
#     fit = model.fit(optimized=True)
#     last_year = int(y.index.max())
#     fut_index = [last_year + i for i in range(1, horizon+1)]
#     fc_values = fit.forecast(horizon)
#     fc = pd.DataFrame({"year": fut_index, "forecast": fc_values.values})
#     return nat_df.copy(), fc

# if not nat.empty and nat["national_index"].notna().any():
#     hist, fc = forecast_national(nat, horizon)
#     nat_fc = hist.merge(fc, on="year", how="outer").sort_values("year")
#     # Ø±Ø³Ù… Ù…Ø´ØªØ±Ùƒ
#     fig_fc = go.Figure()
#     # ØªØ§Ø±ÙŠØ®
#     hist_part = nat_fc[nat_fc["national_index"].notna()]
#     fig_fc.add_trace(go.Scatter(x=hist_part["year"], y=hist_part["national_index"],
#                                 mode="lines+markers", name="ØªØ§Ø±ÙŠØ®"))
#     # ØªÙ†Ø¨Ø¤
#     fc_part = nat_fc[nat_fc["forecast"].notna()]
#     fig_fc.add_trace(go.Scatter(x=fc_part["year"], y=fc_part["forecast"],
#                                 mode="lines+markers", name="ØªÙ†Ø¨Ø¤", line=dict(dash="dash")))
#     fig_fc.update_layout(title=f"ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙˆØ·Ù†ÙŠ Ø­ØªÙ‰ {int(fc_part['year'].max()) if not fc_part.empty else ''}",
#                          yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙˆØ·Ù†ÙŠ (0â€“100)", xaxis_title="Ø§Ù„Ø³Ù†Ø©",
#                          margin=dict(l=10,r=10,t=60,b=10))
#     st.plotly_chart(fig_fc, use_container_width=True)

#     # Ø¨Ø·Ø§Ù‚Ø© Ù…Ù„Ø®Øµ Ù„ØµØ§Ù†Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø±
#     if not fc.empty:
#         st.success(f"**Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ø³Ù†Ø© {int(fc.iloc[0]['year'])}: {fc.iloc[0]['forecast']:.1f}**  â€¢ "
#                    f"**Ø£Ø¹Ù„Ù‰ Ø³Ù†Ø© Ù…ØªÙˆÙ‚Ø¹Ø© Ø¶Ù…Ù† Ø§Ù„Ø£ÙÙ‚: {int(fc.loc[fc['forecast'].idxmax(),'year'])} ({fc['forecast'].max():.1f})**")
# else:
#     st.info("Ø£Ø¶Ù/ØµØ­Ù‘Ø­ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù„Ø§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ÙˆØ·Ù†ÙŠ.")

# # =========================
# # 3.z ØªÙˆØµÙŠØ§Øª Ø³Ø±ÙŠØ¹Ø© Ù„ØµØ§Ù†Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø± (Auto-insights)
# # =========================
# with st.expander("ğŸ’¡ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¢Ù„ÙŠØ© Ù„ØµØ§Ù†Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø±"):
#     if not nat.empty and len(nat) >= 2:
#         last = nat.iloc[-1]["national_index"]
#         prev = nat.iloc[-2]["national_index"]
#         delta = last - prev
#         bullet = []
#         bullet.append(f"- **Ø§Ù„ØªØºÙŠØ± Ø§Ù„Ø³Ù†ÙˆÙŠ Ø§Ù„Ø£Ø®ÙŠØ±**: {delta:+.1f} Ù†Ù‚Ø·Ø©.")
#         if delta < 0:
#             bullet.append("- Ø±ØµØ¯ ØªØ±Ø§Ø¬Ø¹ Ø³Ù†ÙˆÙŠØ› ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª **Leading** Ø§Ù„Ø£Ø¹Ù„Ù‰ ÙˆØ²Ù†Ù‹Ø§.")
#         else:
#             bullet.append("- ØªØ­Ø³Ù† Ø³Ù†ÙˆÙŠØ› Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ÙƒÙØ§Ø¡Ø© Ø§Ù„ØªÙŠ Ø£Ø«Ù‘Ø±Øª Ø¥ÙŠØ¬Ø§Ø¨Ù‹Ø§.")
#         if 'fc' in locals() and not fc.empty:
#             nxt = fc.iloc[0]["forecast"]
#             trend_txt = "Ø§ØªØ¬Ø§Ù‡ ØµØ§Ø¹Ø¯ Ù…ØªÙˆÙ‚Ø¹" if nxt >= last else "Ø§ØªØ¬Ø§Ù‡ Ù‡Ø§Ø¨Ø·/ØªØ³Ø·ÙŠØ­ Ù…ØªÙˆÙ‚Ø¹"
#             bullet.append(f"- **Ø£ÙˆÙ„ Ø³Ù†Ø© Ù…ØªÙˆÙ‚Ø¹Ø©**: {nxt:.1f} â‡’ {trend_txt}.")
#         st.markdown("\n".join(bullet))
#     else:
#         st.write("ØªØ¸Ù‡Ø± Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¨Ø¹Ø¯ ØªÙˆÙØ± Ø³Ù†ØªÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ + Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤.")

# # =========================
# # 4) Ø´Ø±Ø­ ØªÙ†Ø¨Ø¤
# # =========================


# def simple_forecast(nat_df, horizon):
#     y = nat_df.set_index("year")["national_index"].astype(float)
#     xs = np.arange(len(y))
#     A = np.vstack([xs, np.ones(len(xs))]).T
#     m, c = np.linalg.lstsq(A, y.values, rcond=None)[0]

#     last_year = int(y.index.max())
#     fut_years = [last_year + i for i in range(1, horizon+1)]
#     fut_vals = [m*(len(xs)+i-1) + c for i in range(1, horizon+1)]

#     fc = pd.DataFrame({"year": fut_years, "forecast": fut_vals})
#     return nat_df.copy(), fc

# # =========================
# # 4) Ø´Ø±Ø­ Ù…Ø®ØªØµØ±
# # =========================
# with st.expander("Ù…Ø§Ø°Ø§ ÙŠØ¶ÙŠÙ Ù‚Ø³Ù… Ø§Ù„Ø·Ù„Ø§Ø¨ØŸ"):
#     st.markdown("""
# - **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨**: ÙŠØ¹Ø·ÙŠ Ø­Ø¬Ù… Ø§Ù„Ø¹ÙŠÙ†Ø© ÙÙŠ ÙØªØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø®ØªØ§Ø±).
# - **Ø·Ù„Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©**: ÙŠÙÙŠØ¯ ÙÙŠ ØªÙØ³ÙŠØ± ØªØºÙŠÙ‘Ø±Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø± (Ø§Ø±ØªÙØ§Ø¹/Ø§Ù†Ø®ÙØ§Ø¶ Ù‚Ø¯ ÙŠØ±ØªØ¨Ø· Ø¨Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©).
# - **Ø­ØµØ© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù…Ù† Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ**: ØªÙˆØ¶Ø­ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù„Ø¬Ø§Ù…Ø¹Ø© ÙˆÙˆØ²Ù†Ù‡Ø§ Ø§Ù„ÙˆØ·Ù†ÙŠ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†Ø©.
# - **Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠ**: Ù…Ù‚Ø§Ø±Ù†Ø© Ø³Ø±ÙŠØ¹Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª ÙÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ Ù„Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.
# """)

