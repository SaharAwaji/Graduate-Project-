# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FrcjJ_oSu6b4JBvr43v5ITStP4BJei6y
"""

# Library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#!pip install streamlit plotly
import streamlit as st
import plotly.express as px

meta_rows2 = [
    {"indicator_id": "EMPLOY_RATE_6_12M", "name": "Ù…Ø¹Ø¯Ù„ ØªÙˆØ¸ÙŠÙ Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ† Ø®Ù„Ø§Ù„ 6â€“12 Ø´Ù‡Ø±Ù‹Ø§", "component_details": "Outcomes-Employment","component": "Outcomes","component": "Outcomes", "weight": 0.18, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "EMPLOYER_SATISFACTION", "name": "Ù…Ø¹Ø¯Ù„ Ø±Ø¶Ø§ Ø£ØµØ­Ø§Ø¨ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ†", "component_details": "Outcomes-Employment","component": "Outcomes", "weight": 0.14, "function": "Concurrent", "goal": 1, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "PROGRAMS_INTL_RANKED", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…ØµÙ†ÙØ© Ø¯ÙˆÙ„ÙŠÙ‹Ø§", "component_details": "Outcomes-Employment","component": "Outcomes",  "weight": 0.03, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "CAREER_SERVICES", "name": "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø±ÙŠØ§Ø¯ÙŠØ© Ø§Ù„Ù…Ø­ØªØ¶Ù†Ø©/Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©", "component_details": "Outcomes-Innovation& entrepreneurship", "component": "Outcomes", "weight": 0.04, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "PATENT_COUNT", "name": "Ø¹Ø¯Ø¯ Ø¨Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø§Ø®ØªØ±Ø§Ø¹", "component_details": "Outcomes-Innovation& entrepreneurship","component": "Outcomes", "weight": 0.02, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "BIZ_PARTNERSHIPS", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø·Ù„Ø¨Ø© ÙÙŠ Ø¨Ø±Ø§Ù…Ø¬ STEMM (Ù…Ø¹ Ø§Ù„ØµØ­Ø©)", "component_details": "Processes- Education&traning",  "component": "Processess","weight": 0.04, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "NATIONAL_PROF_EXAMS", "name": "Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ ÙÙŠ Ù…ÙˆØ§Ø¯ Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©", "component_details": "Processes- Education&traning", "component": "Processess", "weight": 0.06, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "LINKEDIN_ACTIVE", "name": "Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ®Ø±Ø¬ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯", "component_details": "Processes- Education&traning", "component": "Processess", "weight": 0.06, "function": "Lagging", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "WAGE_GROWTH_1Y", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ© Ø§Ù„Ù…ÙˆØ­Ø¯Ø©", "component_details": "Processes- Education&traning",  "component": "Processess","weight":0.06, "function": "Lagging", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "FOUNDERS_RATE", "name": "Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ÙˆØ·Ù†ÙŠØ©/Ø§Ù„Ù…Ù‡Ù†ÙŠØ©", "component_details": "Processes- Education&traning", "component": "Processess", "weight": 0.05, "function": "Lagging", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "PROMOTION_RATE", "name": "Ø¯Ø¹Ù… Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ† Ø¨Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ù…Ù‡Ù†ÙŠ", "component_details": "Processes- Support & empowerment", "component": "Processess", "weight": 0.04, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "JOB_STABILITY_2Y", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ù…Ø¹ Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„", "component_details": "Processes- Support & empowerment",  "component": "Processess","weight": 0.1, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "STEMM_SHARE", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø·Ù„Ø¨Ø© Ø§Ù„Ù…Ø´Ø§Ø±ÙƒÙŠÙ† ÙÙŠ Ù…Ø¨Ø§Ø¯Ø±Ø§Øª Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±", "component_details": "Processes- Support & empowerment", "component": "Processess", "weight": 0.04, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "ACCRED_PROGS", "name": "Ø±Ø¶Ø§ Ø§Ù„Ø·Ù„Ø¨Ø© Ø¹Ù† Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©", "component_details": "Processes- Support & empowerment", "component": "Processess", "weight": 0.03, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "INTERN_RATE", "name": "Ø±Ø¶Ø§ Ø§Ù„Ø·Ù„Ø¨Ø© Ø¹Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø³Ø§Ù†Ø¯Ø©", "component_details": "Processes- Support & empowerment", "component": "Processess", "weight": 0.03, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "ENTRE_SUPPORT", "name": "Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø§Ù„Ù…Ø¹Ø²Ø²Ø© Ù„Ù…ÙÙ‡ÙˆÙ… Ø±ÙŠØ§Ø¯Ø© Ø§Ù„Ø£Ø¹Ù…Ø§Ù„", "component_details": "Processes-Supportive academic programs","component": "Processes", "weight": 0.03, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "FUNDING_SHARE", "name": "Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ø¨Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø±Ù† 21", "component_details": "Processes-Supportive academic programs",  "component": "Processess","weight": 0.03, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "STUDENT_SAT", "name": "Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ø¨Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¯Ø§Ù…Ø©", "component_details": "Processes-Supportive academic programs", "component": "Processess","weight": 0.03, "function": "Leading", "goal": 3, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "GRAD_SAT", "name": "Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ù‡Ø§Ø±ÙŠ (Skills Portfolio)", "component_details": "Conceptual-Human","component": "Conceptual",  "weight": 0.02, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "SKILL_DEV", "name": "Ø§Ù„Ø´Ù‡Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©", "component_details": "Conceptual-Human", "component": "Conceptual", "weight": 0.02, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "RESEARCH_PUBS", "name": "Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ù…Ø¤ØªÙ…Ø±Ø§Øª/Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø§Øª", "component_details": "Conceptual-Human","component": "Conceptual",  "weight": 0.02, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "INTERNATIONAL_STUD", "name": "Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ø£Ù†Ø¯ÙŠØ© Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠØ©", "component_details": "Conceptual-Social &professional", "component": "Conceptual", "weight": 0.01, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "FACULTY_INTL", "name": "Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„ØªØ·ÙˆØ¹ ÙˆØ®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¬ØªÙ…Ø¹", "component_details": "Conceptual-Social &professional","component": "Conceptual",  "weight": 0.01, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "MOBILITY", "name": "Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ù‡Ù†ÙŠØ© Ø§Ù„Ù†Ø´Ø·Ø© (LinkedIn)", "component_details": "Conceptual-Social &professional", "component": "Conceptual", "weight": 0.04, "function": "Leading", "goal": 1, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "NATIONAL_PART", "name": "ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ùˆ Ù„ØºØ§Øª Ø£Ø¬Ù†Ø¨ÙŠØ©", "component_details": "Conceptual-Human", "component": "Conceptual", "weight": 0.02, "function": "Concurrent", "goal": 2, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "CURRICULUM_UPD", "name": "Ù…ØªÙˆØ³Ø· Ù†Ø³Ø¨Ø© Ø§Ù„Ø²ÙŠØ§Ø¯Ø© ÙÙŠ Ø§Ù„Ø±ÙˆØ§ØªØ¨ Ø¨Ø¹Ø¯ Ø³Ù†Ø©", "component_details": "Impact","component": "Impact",  "weight": 0.12, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "ENTRE_SUPPORT", "name": "Ù†Ø³Ø¨Ø© Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ† Ø§Ù„Ù…Ø¤Ø³Ø³ÙŠÙ† Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø±ÙŠØ§Ø¯ÙŠØ©", "component_details": "Impact","component": "Impact",  "weight": 0.06, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": False},
    {"indicator_id": "EMPLOYABILITY_SKILL", "name": "Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ±Ù‚ÙŠØ§Øª Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ© Ù„Ù„Ø®Ø±ÙŠØ¬ÙŠÙ†", "component_details": "Impact", "component": "Impact", "weight": 0.06, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "COMM_SKILLS", "name": "Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„ÙˆØ¸ÙŠÙÙŠ", "component_details": "Impact","component": "Impact",  "weight": 0.06, "function": "Lagging", "goal": 4, "direction": "HigherIsBetter", "is_likert": True},
    {"indicator_id": "DIGITAL_SKILLS", "name": "Ø§Ù„Ø³Ù…Ø¹Ø© Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ÙŠØ© Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©", "component_details": "Impact","component": "Impact",  "weight": 0.04, "function": "Lagging", "goal": 1, "direction": "HigherIsBetter", "is_likert": True},
]

indicators_meta2 = pd.DataFrame(meta_rows2)
#indicators_meta2

goal_map = {
    1: "Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙˆØ·Ù†ÙŠ",
    2: "ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ÙˆØ§Ù„Ø®Ø¯Ù…Ø§Øª",
    3: "Ø§Ù„ØªØ·ÙˆÙŠØ± ÙˆØ§Ù„Ø§Ø¨ØªÙƒØ§Ø± ÙÙŠ Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬",
    4: "Ø§Ù„Ù‚ÙŠØ§Ø³ ÙˆØ§Ù„Ø£Ø«Ø±"
}

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯
indicators_meta2["goal_name"] = indicators_meta2["goal"].map(goal_map)

# Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØµÙÙˆÙ Ù„Ù„ØªØ£ÙƒØ¯
#print(indicators_meta2.head())
indicators_meta2

# Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø­ÙŠØ« ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ = 1
indicators_meta2["weight"] = indicators_meta2["weight"] / indicators_meta2["weight"].sum()

# Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
print("Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯ =", indicators_meta2["weight"].sum())

indicators_meta2

# # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø·
# # mean_weight = indicators_meta['weight'].mean()
# # indicators_meta['weight'] = indicators_meta2['weight'].fillna(mean_weight)

# # # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
# # indicators_meta['weight'] = indicators_meta['weight'] / indicators_meta['weight'].sum()

# def weighted_mean(values, weights):
#     values = np.asarray(values, dtype=float)
#     weights = np.asarray(weights, dtype=float)
#     if np.all(np.isnan(weights)) or np.nansum(weights) == 0:
#         return float(np.nanmean(values))
#     return float(np.nansum(values * weights) / np.nansum(weights))

np.random.seed(42)
years = [2021, 2022, 2023, 2024]
universities = ["UniA", "UniB", "UniC"]

def likert_to_100(x): return ((x - 1) / 4.0) * 100.0

def minmax_0_100(series, higher_is_better=True):
    s = series.astype(float)
    vmin, vmax = s.min(), s.max()
    norm = (s - vmin) / (vmax - vmin) * 100.0 if vmax > vmin else pd.Series(50.0, index=s.index)
    return norm if higher_is_better else 100.0 - norm


def slope(series):
    years_sorted = sorted(series.keys())
    diffs = [series[b]-series[a] for a,b in zip(years_sorted[:-1], years_sorted[1:])]
    return np.mean(diffs) if diffs else np.nan

def classify_trend(s):
    if pd.isna(s): return "N/A"
    if s >= 5: return "Strong Uptrend"
    if 2 <= s < 5: return "Mild Uptrend"
    if -1.9 <= s <= 1.9: return "Flat"
    if -5 < s < -2: return "Mild Downtrend"
    if s <= -5: return "Strong Downtrend"
    return "Flat"


# -----------------------------
# 3) Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§Ù…
# -----------------------------
raw = []
rng = np.random.default_rng(2025)
for uni in universities:
    for y in years:
        for _, row in indicators_meta2.iterrows():
            if row["is_likert"]:
                val = rng.uniform(2.5, 4.7)  # Ù„ÙŠÙƒØ±Øª 1â€“5
            else:
                val = rng.uniform(40, 95)    # Ù†Ø³Ø¨ Ù…Ø¦ÙˆÙŠØ©
            raw.append((uni, y, row["indicator_id"], val))
raw_df = pd.DataFrame(raw, columns=["university","year","indicator_id","raw_value"])

# -----------------------------
# 4) Ø§Ù„ØªØ·Ø¨ÙŠØ¹
# -----------------------------
norm_rows = []
for ind, sub in raw_df.groupby("indicator_id"):
    meta = indicators_meta2[indicators_meta2["indicator_id"]==ind].iloc[0]
    higher = (meta["direction"] != "LowerIsBetter")
    sub = sub.copy()
    if meta["is_likert"]:
        sub["norm_0_100"] = likert_to_100(sub["raw_value"])
    else:
        sub["norm_0_100"] = minmax_0_100(sub["raw_value"], higher)
    norm_rows.append(sub)
norm_df = pd.concat(norm_rows)

# -----------------------------
# 5) ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
# -----------------------------
merged = norm_df.merge(indicators_meta2,on="indicator_id")
#merged["weight"] = merged["weight"].fillna(0.0)

def aggregate(df, by_cols):
    grouped = []
    for keys, sub in df.groupby(by_cols):
        w = sub["weight"] if "weight" in sub else pd.Series([np.nan]*len(sub))
        v = sub["norm_0_100"].values
        # Ø¥Ø°Ø§ Ù…Ø§ ÙÙŠÙ‡ Ø£ÙŠ ÙˆØ²Ù† â†’ Ù…ØªÙˆØ³Ø· Ø¨Ø³ÙŠØ·
        if w.notna().any():
            val = np.nansum(v * np.nan_to_num(w)) / np.nansum(np.nan_to_num(w))
        else:
            val = np.nanmean(v)
        rec = dict(zip(by_cols, keys if isinstance(keys, tuple) else (keys,)))
        rec["score"] = val
        grouped.append(rec)
    return pd.DataFrame(grouped)

func_level = aggregate(merged, ["university","year","goal","function"])
goal_level = aggregate(func_level.rename(columns={"score":"norm_0_100"}), ["university","year","goal"])
overall = aggregate(goal_level.rename(columns={"score":"norm_0_100"}), ["university","year"]).rename(columns={"score":"overall_index_0_100"})

# -----------------------------
# 6) Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù€ Leading
# -----------------------------
lead = func_level[func_level["function"]=="Leading"]
signals=[]
for (uni,goal),sub in lead.groupby(["university","goal"]):
    s = slope({int(r["year"]):float(r["score"]) for _,r in sub.iterrows()})
    signals.append({"university":uni,"goal":goal,"slope":s,"trend":classify_trend(s),
                    "signal":"Early Signal" if s>=2 else ("Early Warning" if s<=-2 else "Neutral")})
signals_df = pd.DataFrame(signals)

# -----------------------------
# 7) Ø±Ø³ÙˆÙ…
# -----------------------------
latest=max(years)
bar=goal_level[goal_level["year"]==latest].pivot(index="goal",columns="university",values="score")
bar.plot(kind="bar",title=f"Goal Scores {latest}"); plt.ylabel("0â€“100"); plt.show()

for uni in universities:
    mat=goal_level[goal_level["university"]==uni].pivot(index="goal",columns="year",values="score")
    plt.imshow(mat.values,aspect="auto"); plt.colorbar(label="0â€“100")
    plt.title(f"Heatmap - {uni}"); plt.xticks(range(len(mat.columns)),mat.columns); plt.yticks(range(len(mat.index)),mat.index)
    plt.show()

# -----------------------------
# 8) Ø­ÙØ¸ Ù…Ù„Ù Excel
# -----------------------------
# with pd.ExcelWriter("simulation_results.xlsx",engine="xlsxwriter") as writer:
#     indicators_meta.to_excel(writer,index=False,sheet_name="indicators_meta")
#     raw_df.to_excel(writer,index=False,sheet_name="raw_data")
#     norm_df.to_excel(writer,index=False,sheet_name="normalized")
#     func_level.to_excel(writer,index=False,sheet_name="func_level")
#     goal_level.to_excel(writer,index=False,sheet_name="goal_level")
#     overall.to_excel(writer,index=False,sheet_name="overall_index")
#     signals_df.to_excel(writer,index=False,sheet_name="leading_signals")

# --------------------------
# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„ÙƒÙ„ Ù†ÙˆØ¹
# --------------------------
np.random.seed(42)
if 'value' not in indicators_meta2.columns:
    indicators_meta2['value'] = np.random.rand(len(indicators_meta2))

indices_df = indicators_meta2.groupby('function').apply(
    lambda x: (x['value'] * x['weight']).sum()
).reset_index(name='Index')

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
overall_index = indices_df['Index'].sum()
indices_df = pd.concat([indices_df, pd.DataFrame([{'function': 'Overall', 'Index': overall_index}])], ignore_index=True)

print(indices_df)

plt.figure(figsize=(10,6))
colors = {'Leading':'#FFA500', 'Concurrent':'#00BFFF', 'Lagging':'#32CD32', 'Overall':'#FF69B4'}
plt.bar(indices_df['function'], indices_df['Index'], color=[colors[t] for t in indices_df['function']])
plt.title("Overall Index of Graduate Readiness")
plt.ylabel("Weighted Index")
plt.ylim(0, 1)  # Ø¥Ø°Ø§ Ø§Ù„Ù‚ÙŠÙ… Ø¨ÙŠÙ† 0 Ùˆ 1
plt.show()

# --------------------------
# Ø§ÙØªØ±Ø§Ø¶: Ù„Ø¯ÙŠÙƒ DataFrame Ø¨Ø§Ø³Ù… indicators_meta
# ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:
# 'indicator_id', 'name', 'weight', 'value', 'type' (Leading / Concurrent / Lagging)
# --------------------------

# Ù…Ø«Ø§Ù„ Ù„Ù„Ù‚ÙŠÙ… Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
np.random.seed(42)
if 'value' not in indicators_meta2.columns:
    indicators_meta['value'] = np.random.rand(len(indicators_meta2))

# --------------------------
# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙØ±Ø¯ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„ÙˆØ²Ù†
# --------------------------
indicators_meta2['weighted_value'] = indicators_meta2['value'] * indicators_meta2['weight']

# --------------------------
# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„ÙƒÙ„ Ù†ÙˆØ¹
# --------------------------
type_index = indicators_meta2.groupby('function')['weighted_value'].sum().reset_index()
overall_index = type_index['weighted_value'].sum()
type_index = pd.concat([type_index, pd.DataFrame([{'function':'Overall', 'weighted_value':overall_index}])], ignore_index=True)

# --------------------------
# Ø±Ø³Ù… ÙƒÙ„ Ù…Ø¤Ø´Ø± ÙØ±Ø¯ÙŠ Ù…Ø¹ Ù„ÙˆÙ† Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
# --------------------------
plt.figure(figsize=(16,6))
colors = {'Leading':'#FFA500', 'Concurrent':'#00BFFF', 'Lagging':'#32CD32'}

plt.bar(indicators_meta2['indicator_id'], indicators_meta2['weighted_value'],
        color=[colors[t] for t in indicators_meta2['function']]) # Corrected 'type' to 'function'
plt.xticks(rotation=90)
plt.ylabel("Weighted Value")
plt.title("Individual Indicators Weighted Values by Type")
plt.show()

# --------------------------
# Ø±Ø³Ù… Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„ÙƒÙ„ Ù†ÙˆØ¹ + Overall
# --------------------------
plt.figure(figsize=(8,6))
colors_total = {'Leading':'#FFA500', 'Concurrent':'#00BFFF', 'Lagging':'#32CD32', 'Overall':'#FF69B4'}
plt.bar(type_index['function'], type_index['weighted_value'], color=[colors_total[t] for t in type_index['function']])
plt.ylabel("Weighted Index")
plt.title("Overall Index by Indicator Type")
plt.ylim(0, 1)
plt.show()

# --------------------------
# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„ÙŠÙ‡Ø§
# --------------------------
print("Weighted values for individual indicators:")
print(indicators_meta2[['indicator_id','name','function','weighted_value']])
#print(indicators_meta[['indicator_id','name','function','weighted_value']])

print("\nOverall Index by Type:")
print(type_index)

# st.set_page_config(page_title="Overall Graduate Readiness Index", layout="wide")
# st.title("ğŸ“ˆ Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ†")
# st.caption("Gauge Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€¢ Pie Ù„Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ â€¢ Trend Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª")

# # ------------------------------------------------
# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù„ÙŠ ÙƒØªØ¨ØªÙ‡ ÙÙˆÙ‚ (Ù…Ø«Ù„Ø§Ù‹ signals_df, overall, goal_level)
# # Ù‡Ù†Ø§ Ø§ÙØªØ±Ø¶ Ø§Ù†Ùƒ Ø¬Ù‡Ø²ØªÙ‡Ø§ Ù‚Ø¨Ù„
# # ------------------------------------------------

# # Ø¹Ù†ÙˆØ§Ù† Ø±Ø¦ÙŠØ³ÙŠ
# st.title("ğŸ“Š Ù„ÙˆØ­Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© (3-4 Ø³Ù†ÙˆØ§Øª)")

# # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ÙˆØ§Ù„Ù‡Ø¯Ù
# uni_choice = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©:", overall["university"].unique())
# goal_choice = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù‡Ø¯Ù:", goal_level["goal"].unique())

# # -----------------------------
# # 1) Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
# # -----------------------------
# st.subheader("ğŸ“ˆ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ")
# fig_overall = px.line(overall[overall["university"]==uni_choice],
#                       x="year", y="overall_index_0_100", title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„Ù„Ø¬Ø§Ù…Ø¹Ø©")
# st.plotly_chart(fig_overall)

# # -----------------------------
# # 2) Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
# # -----------------------------
# st.subheader("ğŸ¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù")
# goal_sub = goal_level[(goal_level["university"]==uni_choice)]
# fig_goal = px.line(goal_sub, x="year", y="score", color="goal", markers=True,
#                    title=f"ØªØ·ÙˆØ± Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„Ù„Ø¬Ø§Ù…Ø¹Ø© {uni_choice}")
# st.plotly_chart(fig_goal)

# # -----------------------------
# # 3) Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙˆØ¥Ø´Ø§Ø±Ø§Øª Ù…Ø¨ÙƒØ±Ø©
# # -----------------------------
# st.subheader("ğŸš¦ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙˆØ¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ù†Ø°Ø§Ø± Ø§Ù„Ù…Ø¨ÙƒØ±")
# signal_sub = signals_df[signals_df["university"]==uni_choice]
# st.dataframe(signal_sub)

# # -----------------------------
# # 4) ØªÙØµÙŠÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª
# # -----------------------------
# st.subheader(f"ğŸ“‘ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù„Ù„Ù‡Ø¯Ù {goal_choice}")
# func_sub = func_level[(func_level["university"]==uni_choice) & (func_level["goal"]==goal_choice)]
# fig_func = px.line(func_sub, x="year", y="score", color="function", markers=True,
#                    title=f"ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª (Leading, Concurrent, Lagging) Ù„Ù„Ù‡Ø¯Ù {goal_choice}")
# st.plotly_chart(fig_func)

import plotly.graph_objects as go

# st.set_page_config(page_title="Overall Graduate Readiness Index", layout="wide")
# st.title("ğŸ“ˆ Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ†")
# st.caption("Gauge Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€¢ Pie Ù„Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ â€¢ Trend Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª")

# # =========================
# # 0) Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø¬Ø¯Ø§ÙˆÙ„ÙƒØ› ÙˆØ¥Ù† Ù„Ù… ØªØªÙˆÙØ± Ù†ØµÙ†Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
# # =========================


# def weighted_mean(values, weights):
#     values = np.asarray(values, dtype=float)
#     weights = np.asarray(weights, dtype=float)
#     if np.all(np.isnan(weights)) or np.nansum(weights) == 0:
#         return float(np.nanmean(values))
#     return float(np.nansum(values * weights) / np.nansum(weights))

# def build_demo():
#     rng = np.random.default_rng(42)
#     # 30 Ù…Ø¤Ø´Ø± Ù…ÙˆØ²Ø¹Ø©: 8 Leading / 7 Concurrent / 15 Lagging
#     kinds = (["Leading"]*8) + (["Concurrent"]*7) + (["Lagging"]*15)
#     weights = rng.uniform(0.5, 2.0, size=30)
#     weights = weights / weights.sum()
#     rows = []
#     for i in range(30):
#         rows.append({
#             "indicator_id": f"IND_{i+1:02d}",
#             "name": f"Indicator {i+1}",
#             "component_details": "",
#             "component": "",
#             "weight": weights[i],
#             "function": kinds[i],
#             "goal": rng.integers(1, 5),
#             "direction": "HigherIsBetter",
#             "is_likert": bool(rng.random() < 0.25),
#         })
#     indicators_meta2 = pd.DataFrame(rows)

#     universities = ["UniA", "UniB", "UniC"]
#     years = [2021, 2022, 2023, 2024]
#     raw = []
#     for u in universities:
#         for y in years:
#             for _, r in indicators_meta2.iterrows():
#                 val = rng.uniform(2.5, 4.7) if r["is_likert"] else rng.uniform(40, 95)
#                 raw.append((u, y, r["indicator_id"], val))
#     raw_df = pd.DataFrame(raw, columns=["university","year","indicator_id","raw_value"])

#     # ØªØ·Ø¨ÙŠØ¹ Ø¥Ù„Ù‰ 0â€“100 Ù„ÙƒÙ„ Ù…Ø¤Ø´Ø±
#     norm_blocks = []
#     for ind, sub in raw_df.groupby("indicator_id"):
#         meta = indicators_meta2.loc[indicators_meta2["indicator_id"]==ind].iloc[0]
#         sub = sub.copy()
#         if meta["is_likert"]:
#             sub["norm_0_100"] = likert_to_100(sub["raw_value"])
#         else:
#             sub["norm_0_100"] = minmax_0_100(sub["raw_value"], higher_is_better=(meta["direction"]!="LowerIsBetter"))
#         norm_blocks.append(sub)
#     norm_df = pd.concat(norm_blocks, ignore_index=True)

#     merged = norm_df.merge(indicators_meta2, on="indicator_id", how="left")

#     def aggregate(df, by_cols):
#         grouped = []
#         for keys, s in df.groupby(by_cols):
#             w = s["weight"] if "weight" in s else pd.Series([np.nan]*len(s))
#             v = s["norm_0_100"].values
#             val = weighted_mean(v, w)
#             rec = dict(zip(by_cols, keys if isinstance(keys, tuple) else (keys,)))
#             rec["score"] = val
#             grouped.append(rec)
#         return pd.DataFrame(grouped)

#     func_level = aggregate(merged, ["university","year","goal","function"])
#     goal_level = aggregate(func_level.rename(columns={"score":"norm_0_100"}), ["university","year","goal"])
#     overall = aggregate(goal_level.rename(columns={"score":"norm_0_100"}), ["university","year"]).rename(columns={"score":"overall_index_0_100"})

#     return indicators_meta2, raw_df, func_level, goal_level, overall

# # Ø¬Ø±Ù‘Ø¨ Ø£Ø®Ø° Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ù…Ù† Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ (Ø¥Ù† ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©)
# g = globals()
# need_demo = False
# try:
#     indicators_meta2 = g["indicators_meta2"]
#     raw_df = g["raw_df"]
#     func_level = g["func_level"]
#     goal_level = g["goal_level"]
#     overall = g["overall"]
# except KeyError:
#     need_demo = True

# if need_demo:
#     indicators_meta2, raw_df, func_level, goal_level, overall = build_demo()

# # =========================
# # 1) Ø§Ù„Ù…Ø±Ø´Ù‘Ø­Ø§Øª
# # =========================
# left, right = st.columns([1,1])
# with left:
#     universities = sorted(overall["university"].unique().tolist())
#     uni = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©", universities, index=0)
# with right:
#     years = sorted(overall["year"].unique().tolist())
#     year_start, year_end = st.select_slider("Ù†Ø·Ø§Ù‚ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ù„Ù„ØªØ±Ù†Ø¯", options=years, value=(years[0], years[-1]))

# current_year = year_end

# # Ø§Ø·Ø§Ø±Ø§Øª Ù…Ø±Ø´Ù‘Ø­Ø©
# overall_f = overall[(overall["university"]==uni)]
# func_f = func_level[(func_level["university"]==uni)]

# # Ø­Ø³Ø§Ø¨ Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ù„Ø³Ù†Ø© Ù…Ø¹ÙŠÙ‘Ù†Ø© (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†ÙˆØ¹ ÙƒØ­ØµØ© Ù…Ù† Ø§Ù„ÙƒÙ„ÙŠ)
# # Ù†Ø¬Ù…Ø¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù…Ù† meta Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†ÙˆØ¹:
# type_weights = indicators_meta2.groupby("function", as_index=False)["weight"].sum().rename(columns={"weight":"w_sum"})

# # Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†ÙˆØ¹ = Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¬Ù‘Ø­ (Ù…Ø­Ø³ÙˆØ¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙÙŠ func_level.score)Ø›
# # Ø³Ù†Ø³ØªØ®Ø¯Ù… w_sum ÙÙ‚Ø· Ù„Ø¹Ø±Ø¶ "Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø©" Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙ„ÙŠ.
# type_score_now = func_f[func_f["year"]==current_year].groupby("function", as_index=False)["score"].mean()
# type_breakdown = type_weights.merge(type_score_now, on="function", how="left").fillna({"score":0})

# # Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„Ø³Ù†Ø© Ø§Ù„Ø¹Ø±Ø¶
# overall_now = overall_f[overall_f["year"]==current_year]["overall_index_0_100"]
# overall_now = float(overall_now.iloc[0]) if len(overall_now) else np.nan

# # =========================
# # 2) Visualization
# # =========================
# c1, c2 = st.columns([1,1])

# # 2.1 Gauge (Speedometer) Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
# with c1:
#     st.subheader("Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)")
#     if pd.notna(overall_now):
#         gauge = go.Figure(go.Indicator(
#             mode="gauge+number",
#             value=overall_now,
#             number={"suffix":" / 100"},
#             title={"text": f"Overall Index â€“ {uni} ({current_year})"},
#             gauge={
#                 "axis":{"range":[0,100]},
#                 "bar":{"thickness":0.2},
#                 "steps":[
#                     {"range":[0,50],"color":"#fce4e4"},
#                     {"range":[50,75],"color":"#fff3cd"},
#                     {"range":[75,100],"color":"#e8f5e9"},
#                 ]
#             }
#         ))
#         gauge.update_layout(height=360, margin=dict(l=20,r=20,t=50,b=10))
#         st.plotly_chart(gauge, use_container_width=True)
#     else:
#         st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠÙ…Ø© Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.")

# # 2.2 Pie â€“ Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙ„ÙŠ (Ø­Ø³Ø¨ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†)
# with c2:
#     st.subheader("Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ")
#     pie_df = type_breakdown.copy()
#     pie_df["function"] = pie_df["function"].map({
#         "Leading":"Leading (Ø¥Ù†Ø°Ø§Ø± Ù…Ø¨ÙƒØ±)",
#         "Concurrent":"Concurrent (ØªØ´Ø®ÙŠØµ ÙÙˆØ±ÙŠ)",
#         "Lagging":"Lagging (Ù‚ÙŠØ§Ø³ Ø£Ø«Ø±)"
#     }).fillna(pie_df["function"])

#     pie = px.pie(pie_df, names="function", values="w_sum", hole=0.45,
#                  title=f"ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹ â€“ {uni}")
#     pie.update_layout(legend_title="", height=360, margin=dict(l=10,r=10,t=60,b=10))
#     st.plotly_chart(pie, use_container_width=True)

# # 2.3 Trend line â€“ ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª
# st.markdown("### Ø§Ù„ØªØ±Ù†Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ")
# trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year")
# if not trend.empty and trend["overall_index_0_100"].notna().any():
#     line = px.line(trend, x="year", y="overall_index_0_100", markers=True,
#                    title=f"ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€“ {uni}")
#     line.update_layout(yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)", xaxis_title="Ø§Ù„Ø³Ù†Ø©")
#     st.plotly_chart(line, use_container_width=True)
# else:
#     st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØ±Ù†Ø¯ ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯.")

# # =========================
# # 3) Ù„Ù‚Ø·Ø§Øª ØªÙØ³ÙŠØ±ÙŠØ©
# # =========================
# with st.expander("Ù…Ø§Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ØŸ"):
#     st.markdown("""
# - **Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (Gauge)**: Ø¯Ø±Ø¬Ø© Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ† Ø¹Ù„Ù‰ Ù…Ù‚ÙŠØ§Ø³ 0â€“100 Ø¨Ø¹Ø¯ **ØªØ·Ø¨ÙŠØ¹** Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ© ÙˆØªØ¬Ù…ÙŠØ¹Ù‡Ø§ Ø¨ÙˆØ²Ù† ÙƒÙ„ Ù…Ø¤Ø´Ø±.
# - **Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ (Pie)**: ØªØ¹Ø·ÙŠÙƒ **Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¤Ø´Ø±**â€”ÙƒÙ… ÙŠØ´ÙƒÙ‘Ù„ **Leading/Concurrent/Lagging** Ù…Ù† Ø§Ù„ÙˆØ²Ù† Ø§Ù„ÙƒÙ„ÙŠØ›
#   Ù‡Ø°Ø§ ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ ÙÙ‡Ù…: Ù‡Ù„ Ø§Ù„Ù…Ø¤Ø´Ø± ÙŠØ¹ØªÙ…Ø¯ Ø£ÙƒØ«Ø± Ø¹Ù„Ù‰ Ø¥Ø´Ø§Ø±Ø§Øª Ù…Ø¨ÙƒØ±Ø© Ø£Ù… Ø¹Ù„Ù‰ Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø«Ø± Ù„Ø§Ø­Ù‚Ø©ØŸ
# - **Ø§Ù„ØªØ±Ù†Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠ (Line)**: ÙŠÙˆØ¶Ù‘Ø­ **Ù…Ø³Ø§Ø± Ø§Ù„ØªØ­Ø³Ù‘Ù† Ø£Ùˆ Ø§Ù„ØªØ±Ø§Ø¬Ø¹** Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø§Ø³ØªØ¯Ø§Ù…Ø© ÙˆÙ„ÙŠØ³ Ù„Ù‚Ø·Ø© Ù„Ø­Ø¸ÙŠØ© ÙÙ‚Ø·.
#     """)

st.set_page_config(page_title="Overall Graduate Readiness Index", layout="wide")
st.title("ğŸ“ˆ Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ù„Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ø®Ø±ÙŠØ¬ÙŠÙ†")
st.caption("Gauge Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€¢ Pie Ù„Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ â€¢ Trend Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª â€¢ ğŸ‘¨â€ğŸ“ Ø¹Ø¯Ù‘Ø§Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨")

# =========================
# 0) Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© + Ø¨ÙŠØ§Ù†Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
# =========================
def likert_to_100(x):
    return ((x - 1.0) / 4.0) * 100.0  # 1..5 -> 0..100

def minmax_0_100(series, higher_is_better=True):
    s = series.astype(float)
    mn, mx = s.min(), s.max()
    scaled = (s - mn) / (mx - mn) * 100.0 if mx > mn else pd.Series(50.0, index=s.index)
    return scaled if higher_is_better else 100.0 - scaled

def weighted_mean(values, weights):
    values = np.asarray(values, dtype=float)
    weights = np.asarray(weights, dtype=float)
    if np.all(np.isnan(weights)) or np.nansum(weights) == 0:
        return float(np.nanmean(values))
    return float(np.nansum(values * weights) / np.nansum(weights))

def build_demo():
    rng = np.random.default_rng(42)
    # 30 Ù…Ø¤Ø´Ø±: 8 Leading / 7 Concurrent / 15 Lagging
    kinds = (["Leading"]*8) + (["Concurrent"]*7) + (["Lagging"]*15)
    weights = rng.uniform(0.5, 2.0, size=30); weights = weights / weights.sum()
    rows = []
    for i in range(30):
        rows.append({
            "indicator_id": f"IND_{i+1:02d}",
            "name": f"Indicator {i+1}",
            "component_details": "",
            "component": "",
            "weight": weights[i],
            "function": kinds[i],
            "goal": rng.integers(1, 5),
            "direction": "HigherIsBetter",
            "is_likert": bool(rng.random() < 0.25),
        })
    indicators_meta2 = pd.DataFrame(rows)

    universities = ["UniA", "UniB", "UniC"]
    years = [2021, 2022, 2023, 2024]
    raw = []
    for u in universities:
        for y in years:
            for _, r in indicators_meta2.iterrows():
                val = rng.uniform(2.5, 4.7) if r["is_likert"] else rng.uniform(40, 95)
                raw.append((u, y, r["indicator_id"], val))
    raw_df = pd.DataFrame(raw, columns=["university","year","indicator_id","raw_value"])

    # ØªØ·Ø¨ÙŠØ¹ 0â€“100 Ù„ÙƒÙ„ Ù…Ø¤Ø´Ø±
    norm_blocks = []
    for ind, sub in raw_df.groupby("indicator_id"):
        meta = indicators_meta2.loc[indicators_meta2["indicator_id"]==ind].iloc[0]
        sub = sub.copy()
        if meta["is_likert"]:
            sub["norm_0_100"] = likert_to_100(sub["raw_value"])
        else:
            sub["norm_0_100"] = minmax_0_100(sub["raw_value"], higher_is_better=(meta["direction"]!="LowerIsBetter"))
        norm_blocks.append(sub)
    norm_df = pd.concat(norm_blocks, ignore_index=True)

    merged = norm_df.merge(indicators_meta2, on="indicator_id", how="left")

    def aggregate(df, by_cols):
        grouped = []
        for keys, s in df.groupby(by_cols):
            w = s["weight"] if "weight" in s else pd.Series([np.nan]*len(s))
            v = s["norm_0_100"].values
            val = weighted_mean(v, w)
            rec = dict(zip(by_cols, keys if isinstance(keys, tuple) else (keys,)))
            rec["score"] = val
            grouped.append(rec)
        return pd.DataFrame(grouped)

    func_level = aggregate(merged, ["university","year","goal","function"])
    goal_level = aggregate(func_level.rename(columns={"score":"norm_0_100"}), ["university","year","goal"])
    overall = aggregate(goal_level.rename(columns={"score":"norm_0_100"}), ["university","year"]).rename(columns={"score":"overall_index_0_100"})

    # Ø·Ù„Ø§Ø¨ ØªØ¬Ø±ÙŠØ¨ÙŠ: ØªÙˆØ²ÙŠØ¹ Ø¹Ø´ÙˆØ§Ø¦ÙŠ ÙˆØ§Ù‚Ø¹ÙŠ
    srows = []
    for y in years:
        # Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆØ·Ù†ÙŠ ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„ÙƒÙ„ Ø³Ù†Ø©
        national_total = int(rng.integers(24000, 36000))
        # Ù†Ø³Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª
        shares = rng.random(3); shares = shares / shares.sum()
        for u, sh in zip(universities, shares):
            srows.append((u, y, int(national_total * sh)))
    students_df = pd.DataFrame(srows, columns=["university","year","students"])

    return indicators_meta2, raw_df, func_level, goal_level, overall, students_df

g = globals()
need_demo = False
try:
    indicators_meta2 = g["indicators_meta2"]
    raw_df = g["raw_df"]
    func_level = g["func_level"]
    goal_level = g["goal_level"]
    overall = g["overall"]
except KeyError:
    need_demo = True

if need_demo:
    indicators_meta2, raw_df, func_level, goal_level, overall, students_df = build_demo()
else:
    # Ù„Ùˆ Ø¹Ù†Ø¯Ù†Ø§ Ù…Ù„Ù Ø·Ù„Ø§Ø¨ Ù†Ù‚Ø¯Ø± Ù†Ù‚Ø±Ø£Ù‡ Ù…Ù† Ù‡Ù†Ø§:
    students_file = st.sidebar.file_uploader("ğŸ“¥ Ø­Ù…Ù‘Ù„ Ù…Ù„Ù Ø§Ù„Ø·Ù„Ø§Ø¨ (students.csv)", type=["csv"])
    if students_file is not None:
        students_df = pd.read_csv(students_file)
    else:
        # ÙÙŠ Ø­Ø§Ù„ Ù…Ø§ Ø¹Ù†Ø¯Ù†Ø§ Ù…Ù„Ù Ø²ÙŠ Ø­Ø§Ù„ØªÙ†Ø§ Ø§Ù„Ø­ÙŠÙ†ØŒ Ù†ØµÙ†Ø¹ Ø¯Ø§ØªØ§ Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ù† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª/Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©
        _, _, _, _, _, students_df = build_demo()

# =========================
# 1) Ø§Ù„Ù…Ø±Ø´Ù‘Ø­Ø§Øª
# =========================
left, right = st.columns([1,1])
with left:
    universities = sorted(overall["university"].unique().tolist())
    uni = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©", universities, index=0)
with right:
    years = sorted(overall["year"].unique().tolist())
    year_start, year_end = st.select_slider("Ù†Ø·Ø§Ù‚ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ù„Ù„ØªØ±Ù†Ø¯", options=years, value=(years[0], years[-1]))

current_year = year_end

# Ø§Ø·Ø§Ø±Ø§Øª Ù…Ø±Ø´Ù‘Ø­Ø©
overall_f = overall[(overall["university"]==uni)]
func_f = func_level[(func_level["university"]==uni)]

# Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ (Ø­Ø³Ø¨ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†)
type_weights = indicators_meta2.groupby("function", as_index=False)["weight"].sum().rename(columns={"weight":"w_sum"})
type_score_now = func_f[func_f["year"]==current_year].groupby("function", as_index=False)["score"].mean()
type_breakdown = type_weights.merge(type_score_now, on="function", how="left").fillna({"score":0})

# Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
overall_now = overall_f[overall_f["year"]==current_year]["overall_index_0_100"]
overall_now = float(overall_now.iloc[0]) if len(overall_now) else np.nan

# =========================
# 2) ğŸ‘¨â€ğŸ“ Ù‚Ø³Ù… Ø¹Ø¯Ù‘Ø§Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨
# =========================
st.markdown("### ğŸ‘¨â€ğŸ“ Ø§Ù„Ø·Ù„Ø§Ø¨")
# ØªØµÙÙŠØ© Ø§Ù„Ø·Ù„Ø§Ø¨ Ø­Ø³Ø¨ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø³Ù†ÙˆØ§Øª
students_in_range = students_df[(students_df["year"]>=year_start) & (students_df["year"]<=year_end)]
# Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ (ÙˆØ·Ù†ÙŠ) Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ø·Ø§Ù‚
national_total_range = int(students_in_range["students"].sum())
# Ø·Ù„Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ø®ØªØ§Ø±Ø© ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
uni_current_students = int(
    students_df[(students_df["university"]==uni) & (students_df["year"]==current_year)]["students"].sum()
) if not students_df.empty else 0

m1, m2, m3 = st.columns(3)
with m1:
    st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ (ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø®ØªØ§Ø±)", f"{national_total_range:,}")
with m2:
    st.metric(f"Ø·Ù„Ø§Ø¨ {uni} ÙÙŠ {current_year}", f"{uni_current_students:,}")
with m3:
    # Ø­ØµØ© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù…Ù† Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†Ø©
    national_current_total = int(students_df[students_df["year"]==current_year]["students"].sum())
    share = (uni_current_students / national_current_total * 100.0) if national_current_total>0 else 0.0
    st.metric("Ø­ØµØ© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ù†Ø©", f"{share:.1f}%")

# ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
st.write(f"**ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© â€“ {current_year}**")
students_current_year = students_df[students_df["year"]==current_year].copy()
if not students_current_year.empty:
    bar_students = px.bar(students_current_year, x="university", y="students", text="students",
                          title=f"Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ({current_year})")
    bar_students.update_layout(yaxis_title="Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨", xaxis_title="Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©")
    bar_students.update_traces(texttemplate="%{text:,}", textposition="outside")
    st.plotly_chart(bar_students, use_container_width=True)
else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø·Ù„Ø§Ø¨ Ù„Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.")

# =========================
# 3) Visualization Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
# =========================
c1, c2 = st.columns([1,1])

# 3.1 Gauge (Speedometer) Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ
with c1:
    st.subheader("Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)")
    if pd.notna(overall_now):
        gauge = go.Figure(go.Indicator(
            mode="gauge+number",
            value=overall_now,
            number={"suffix":" / 100"},
            title={"text": f"Overall Index â€“ {uni} ({current_year})"},
            gauge={
                "axis":{"range":[0,100]},
                "bar":{"thickness":0.2},
                "steps":[
                    {"range":[0,50],"color":"#fce4e4"},
                    {"range":[50,75],"color":"#fff3cd"},
                    {"range":[75,100],"color":"#e8f5e9"},
                ]
            }
        ))
        gauge.update_layout(height=360, margin=dict(l=20,r=20,t=50,b=10))
        st.plotly_chart(gauge, use_container_width=True)
    else:
        st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠÙ…Ø© Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.")

# 3.2 Pie â€“ Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙ„ÙŠ (Ø­Ø³Ø¨ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†)
with c2:
    st.subheader("Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ")
    pie_df = type_breakdown.copy()
    pie_df["function"] = pie_df["function"].map({
        "Leading":"Leading (Ø¥Ù†Ø°Ø§Ø± Ù…Ø¨ÙƒØ±)",
        "Concurrent":"Concurrent (ØªØ´Ø®ÙŠØµ ÙÙˆØ±ÙŠ)",
        "Lagging":"Lagging (Ù‚ÙŠØ§Ø³ Ø£Ø«Ø±)"
    }).fillna(pie_df["function"])

    pie = px.pie(pie_df, names="function", values="w_sum", hole=0.45,
                 title=f"ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹ â€“ {uni}")
    pie.update_layout(legend_title="", height=360, margin=dict(l=10,r=10,t=60,b=10))
    st.plotly_chart(pie, use_container_width=True)

# 3.3 Trend line â€“ ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª
st.markdown("### Ø§Ù„ØªØ±Ù†Ø¯ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ")
trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year")
if not trend.empty and trend["overall_index_0_100"].notna().any():
    line = px.line(trend, x="year", y="overall_index_0_100", markers=True,
                   title=f"ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€“ {uni}")
    line.update_layout(yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)", xaxis_title="Ø§Ù„Ø³Ù†Ø©")
    st.plotly_chart(line, use_container_width=True)
else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØ±Ù†Ø¯ ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯.")


# 3.4 Ø¨Ø¯ÙŠÙ„: Ø¹Ù…ÙˆØ¯ÙŠØ§Øª Ù…Ø¹ ØªØ³Ù…ÙŠØ§Øª
st.markdown("### ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª")
trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year")
if not trend.empty and trend["overall_index_0_100"].notna().any():
    bar = px.bar(
        trend, x="year", y="overall_index_0_100",
        title=f"ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ â€“ {uni}", text="overall_index_0_100"
    )
    bar.update_traces(texttemplate="%{text:.1f}", textposition="outside")
    bar.update_layout(
        yaxis_title="Ø§Ù„Ù…Ø¤Ø´Ø± Ø§Ù„ÙƒÙ„ÙŠ (0â€“100)",
        xaxis_title="Ø§Ù„Ø³Ù†Ø©",
        uniformtext_minsize=10, uniformtext_mode='hide',
        margin=dict(l=10, r=10, t=60, b=10)
    )
    st.plotly_chart(bar, use_container_width=True)
else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØ±Ù†Ø¯ ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯.")

# 3.5 Ø¨Ø¯ÙŠÙ„: ÙØ±ÙˆÙ‚Ø§Øª Ø³Ù†Ø© Ø¨Ø³Ù†Ø©
st.markdown("### Ø§Ù„ØªØºÙŠØ± Ø³Ù†Ø© Ø¨Ø³Ù†Ø© (YoY)")
trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year")
if len(trend) >= 2 and trend["overall_index_0_100"].notna().any():
    trend["yoy_delta"] = trend["overall_index_0_100"].diff()
    yoy = trend.dropna(subset=["yoy_delta"])
    # ØªÙ„ÙˆÙŠÙ† Ø­Ø³Ø¨ Ù…ÙˆØ¬Ø¨/Ø³Ø§Ù„Ø¨
    yoy["direction"] = np.where(yoy["yoy_delta"]>=0, "ØªØ­Ø³Ù†", "ØªØ±Ø§Ø¬Ø¹")
    bar_yoy = px.bar(
        yoy, x="year", y="yoy_delta", color="direction",
        color_discrete_map={"ØªØ­Ø³Ù†":"#2ca02c","ØªØ±Ø§Ø¬Ø¹":"#d62728"},
        title=f"Ø§Ù„ØªØºÙŠØ± Ø³Ù†Ø© Ø¨Ø³Ù†Ø© â€“ {uni}", text="yoy_delta"
    )
    bar_yoy.update_traces(texttemplate="%{text:.1f}", textposition="outside")
    bar_yoy.update_layout(yaxis_title="ÙØ±Ù‚ Ø§Ù„Ù†Ù‚Ø§Ø· Ø¹Ù† Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©", xaxis_title="Ø§Ù„Ø³Ù†Ø©")
    st.plotly_chart(bar_yoy, use_container_width=True)
else:
    st.info("ÙŠÙ„Ø²Ù… ØªÙˆÙØ± Ø³Ù†ØªÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªØºÙŠØ± Ø³Ù†Ø© Ø¨Ø³Ù†Ø©.")

# 3.6 Ø¨Ø¯ÙŠÙ„: Sparkline + Ù…Ù„Ø®Øµ
st.markdown("### Ù„Ù…Ø­Ø© Ø³Ø±ÙŠØ¹Ø© Ø¹Ù† Ø§Ù„Ø§ØªØ¬Ø§Ù‡")
trend = overall_f[(overall_f["year"]>=year_start) & (overall_f["year"]<=year_end)].sort_values("year")
if not trend.empty and trend["overall_index_0_100"].notna().any():
    last = trend.iloc[-1]["overall_index_0_100"]
    prev = trend.iloc[-2]["overall_index_0_100"] if len(trend) >= 2 else np.nan
    delta = (last - prev) if pd.notna(prev) else np.nan

    k1, k2 = st.columns([1,2])
    with k1:
        st.metric("Ø¢Ø®Ø± Ù‚ÙŠÙ…Ø©", f"{last:.1f}", delta=None if pd.isna(delta) else f"{delta:+.1f}")
    with k2:
        spark = px.line(trend, x="year", y="overall_index_0_100")
        spark.update_layout(
            height=140, margin=dict(l=10,r=10,t=10,b=10),
            yaxis_title=None, xaxis_title=None
        )
        spark.update_traces(mode="lines+markers")
        st.plotly_chart(spark, use_container_width=True)
else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„Ø®Øµ.")

# =========================
# 4) Ø´Ø±Ø­ Ù…Ø®ØªØµØ±
# =========================
with st.expander("Ù…Ø§Ø°Ø§ ÙŠØ¶ÙŠÙ Ù‚Ø³Ù… Ø§Ù„Ø·Ù„Ø§Ø¨ØŸ"):
    st.markdown("""
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨**: ÙŠØ¹Ø·ÙŠ Ø­Ø¬Ù… Ø§Ù„Ø¹ÙŠÙ†Ø© ÙÙŠ ÙØªØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø®ØªØ§Ø±).
- **Ø·Ù„Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©**: ÙŠÙÙŠØ¯ ÙÙŠ ØªÙØ³ÙŠØ± ØªØºÙŠÙ‘Ø±Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø± (Ø§Ø±ØªÙØ§Ø¹/Ø§Ù†Ø®ÙØ§Ø¶ Ù‚Ø¯ ÙŠØ±ØªØ¨Ø· Ø¨Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©).
- **Ø­ØµØ© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù…Ù† Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ**: ØªÙˆØ¶Ø­ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù„Ø¬Ø§Ù…Ø¹Ø© ÙˆÙˆØ²Ù†Ù‡Ø§ Ø§Ù„ÙˆØ·Ù†ÙŠ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ù†Ø©.
- **Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠ**: Ù…Ù‚Ø§Ø±Ù†Ø© Ø³Ø±ÙŠØ¹Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª ÙÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ Ù„Ù„Ø³Ù†Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.
""")